{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import json\n",
        "config_file = r\"21_load\\db_config.json\"\n",
        "def get_db_connection(config_path=config_file, database_override=None):\n",
        "    \"\"\"\n",
        "    Returns a psycopg2 connection object using the given config file.\n",
        "    You can override the database name by passing `database_override`.\n",
        "    \"\"\"\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    if database_override:\n",
        "        config[\"database\"] = database_override\n",
        "\n",
        "    conn = psycopg2.connect(**config)\n",
        "    return conn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from psycopg2 import connect, sql, errors\n",
        "import json\n",
        "config_file = r\"21_load\\db_config.json\"\n",
        "def load_config(config_path=config_file):\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def database_exists(db_name, config_path=config_file):\n",
        "    config = load_config(config_path)\n",
        "    conn = connect(database=\"postgres\", user=config[\"user\"], password=config[\"password\"],\n",
        "                   host=config[\"host\"], port=config[\"port\"])\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (db_name,))\n",
        "        return cur.fetchone() is not None\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "def create_database(db_name, config_path=config_file):\n",
        "    config = load_config(config_path)\n",
        "    conn = connect(database=\"postgres\", user=config[\"user\"], password=config[\"password\"],\n",
        "                   host=config[\"host\"], port=config[\"port\"])\n",
        "    conn.autocommit = True\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        cur.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(db_name)))\n",
        "        print(f\"Database '{db_name}' created.\")\n",
        "    except errors.DuplicateDatabase:\n",
        "        print(f\"Database '{db_name}' already exists.\")\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "def delete_database(db_name, config_path=config_file):\n",
        "    config = load_config(config_path)\n",
        "    conn = connect(database=\"postgres\", user=config[\"user\"], password=config[\"password\"],\n",
        "                   host=config[\"host\"], port=config[\"port\"])\n",
        "    conn.autocommit = True\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        cur.execute(sql.SQL(\"DROP DATABASE {} \").format(sql.Identifier(db_name)))\n",
        "        print(f\"Database '{db_name}' deleted.\")\n",
        "    except errors.InvalidCatalogName:\n",
        "        print(f\"Database '{db_name}' does not exist.\")\n",
        "    except errors.ObjectInUse:\n",
        "        print(f\"Cannot drop '{db_name}': it is currently being accessed.\")\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'database_exists' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Create DB 4M:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdatabase_exists\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4M\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      4\u001b[0m     create_database(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4M\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'database_exists' is not defined"
          ]
        }
      ],
      "source": [
        "## Create DB 4M:\n",
        "\n",
        "if not database_exists(\"4M\"):\n",
        "    create_database(\"4M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database '4M' deleted.\n"
          ]
        }
      ],
      "source": [
        "## Delete DB 4M:\n",
        "\n",
        "if database_exists(\"4M\"):\n",
        "     delete_database(\"4M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cannot drop '4M': it is currently being accessed.\n",
            "Error executing SQL script: relation \"merged_dataset_metadata\" already exists\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Delete DB 4M:\n",
        "\n",
        "if database_exists(\"4M\"):\n",
        "     delete_database(\"4M\")\n",
        "\n",
        "## Create DB 4M:\n",
        "\n",
        "if not database_exists(\"4M\"):\n",
        "    create_database(\"4M\")\n",
        "\n",
        "    \n",
        "config_file = r\"21_load\\db_config.json\"\n",
        "\n",
        "def execute_sql_script(db_name, sql_file_path, config_path=config_file):\n",
        "    config = load_config(config_path)\n",
        "    conn = connect(database=db_name, user=config[\"user\"], password=config[\"password\"],\n",
        "                   host=config[\"host\"], port=config[\"port\"])\n",
        "    conn.autocommit = True\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        with open(sql_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            sql_script = f.read()\n",
        "        cur.execute(sql_script)\n",
        "        print(f\"SQL script '{sql_file_path}' executed successfully on database '{db_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing SQL script: {e}\")\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "execute_sql_script(\"4M\", \"21_load/create_db_script.sql\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading CSV: duplicate key value violates unique constraint \"merged_dataset_metadata_pkey\"\n",
            "DETAIL:  Key (dataset_identifier)=(da4696e6-c546-4a4f-bbf4-36d5d01c9e2f-6571@agis_service_center) already exists.\n",
            "CONTEXT:  COPY merged_dataset_metadata, line 2\n",
            "\n",
            "CSV '11_group\\merged_contact_point_metadata.csv' loaded into table 'merged_contact_point_metadata'.\n",
            "CSV '11_group\\merged_distribution_metadata.csv' loaded into table 'merged_distribution_metadata'.\n"
          ]
        }
      ],
      "source": [
        "def load_csv_to_table(db_name, table_name, csv_path, config_path=config_file):\n",
        "    config = load_config(config_path)\n",
        "    conn = connect(database=db_name, user=config[\"user\"], password=config[\"password\"],\n",
        "                   host=config[\"host\"], port=config[\"port\"])\n",
        "    conn.autocommit = True\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
        "            cur.copy_expert(\n",
        "                sql.SQL(\"COPY {} FROM STDIN WITH CSV HEADER\").format(sql.Identifier(table_name)),\n",
        "                f\n",
        "            )\n",
        "        print(f\"CSV '{csv_path}' loaded into table '{table_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "csv_path_dataset = r\"11_group\\merged_dataset_metadata.csv\"\n",
        "load_csv_to_table(\"4M\", \"merged_dataset_metadata\", csv_path_dataset)\n",
        "\n",
        "csv_path_contact_point = r\"11_group\\merged_contact_point_metadata.csv\"\n",
        "load_csv_to_table(\"4M\", \"merged_contact_point_metadata\", csv_path_contact_point)\n",
        "\n",
        "csv_path_distribution = r\"11_group\\merged_distribution_metadata.csv\"\n",
        "load_csv_to_table(\"4M\", \"merged_distribution_metadata\", csv_path_distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.json', 'http://publications.europa.eu/resource/authority/file-type/JSON')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Web Map Tile Service', 'http://publications.europa.eu/resource/authority/file-type/XML')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Web Feature Service', 'http://publications.europa.eu/resource/authority/file-type/WFS_SRVC')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Web Map Service', 'http://publications.europa.eu/resource/authority/file-type/WMS_SRVC')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.csv', 'http://publications.europa.eu/resource/authority/file-type/CSV')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.dxf', 'http://publications.europa.eu/resource/authority/file-type/DXF')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.json (GeoJSON-Services)', 'http://publications.europa.eu/resource/authority/file-type/JSON')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.gpkg', 'http://publications.europa.eu/resource/authority/file-type/GPKG')\n",
            "('c771f5e3-14bf-4549-87f8-9e2270842eaa@stadt-zurich', 'zuri-wc.xml', 'Open Data ZÃ¼rich', 'Zueri_WC.shp', 'http://publications.europa.eu/resource/authority/file-type/SHP')\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "import json\n",
        "\n",
        "config_file = r\"21_load\\\\db_config.json\"\n",
        "\n",
        "def load_config(config_path=config_file):\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def fetch_metadata_details(db_name, xml_filename):\n",
        "    \"\"\"\n",
        "    Execute a query to fetch metadata details for a specific XML file.\n",
        "\n",
        "    Args:\n",
        "        db_name (str): PostgreSQL database name.\n",
        "        xml_filename (str): XML filename to filter by.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    SELECT\n",
        "        d.dataset_identifier,\n",
        "        d.xml_filename,\n",
        "        c.contact_name,\n",
        "        dist.distribution_title_DE,\n",
        "        dist.distribution_format\n",
        "    FROM merged_dataset_metadata d\n",
        "    LEFT JOIN merged_contact_point_metadata c\n",
        "        ON d.dataset_identifier = c.dataset_identifier\n",
        "    LEFT JOIN merged_distribution_metadata dist\n",
        "        ON d.dataset_identifier = dist.dataset_identifier\n",
        "    WHERE d.xml_filename = %s;\n",
        "    \"\"\"\n",
        "\n",
        "    config = load_config()\n",
        "\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=db_name,\n",
        "            user=config[\"user\"],\n",
        "            password=config[\"password\"],\n",
        "            host=config[\"host\"],\n",
        "            port=config[\"port\"]\n",
        "        )\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query, (xml_filename,))\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching metadata details\", exception=e)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "fetch_metadata_details(\"4M\", \"zuri-wc.xml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import json\n",
        "\n",
        "config_file = r\"21_load\\\\db_config.json\"\n",
        "\n",
        "def load_config(config_path=config_file):\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def fetch_metadata_details(db_name, xml_filename):\n",
        "    \"\"\"\n",
        "    Execute a query to fetch metadata details for a specific XML file.\n",
        "\n",
        "    Args:\n",
        "        db_name (str): PostgreSQL database name.\n",
        "        xml_filename (str): XML filename to filter by.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "        SELECT * FROM merged_distribution_metadata WHERE xml_filename = %s;\n",
        "    \"\"\"\n",
        "\n",
        "    config = load_config()\n",
        "\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=db_name,\n",
        "            user=config[\"user\"],\n",
        "            password=config[\"password\"],\n",
        "            host=config[\"host\"],\n",
        "            port=config[\"port\"]\n",
        "        )\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query, (xml_filename,))\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching metadata details\", exception=e)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "fetch_metadata_details(\"4M\", \"zuri-wc.xml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted 1 dataset(s) and all related metadata entries for xml_filename = 'zuri-wc.xml'\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "import json\n",
        "\n",
        "config_file = r\"21_load\\\\db_config.json\"\n",
        "\n",
        "def load_config(config_path=config_file):\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def delete_metadata_by_filename(db_name, xml_filename):\n",
        "    \"\"\"\n",
        "    Delete entries from metadata tables by finding dataset_identifier(s) for the given xml_filename,\n",
        "    then deleting from the root dataset table, cascading to children via foreign keys.\n",
        "\n",
        "    Args:\n",
        "        db_name (str): PostgreSQL database name.\n",
        "        xml_filename (str): XML filename to match.\n",
        "    \"\"\"\n",
        "    config = load_config()\n",
        "\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=db_name,\n",
        "            user=config[\"user\"],\n",
        "            password=config[\"password\"],\n",
        "            host=config[\"host\"],\n",
        "            port=config[\"port\"]\n",
        "        )\n",
        "        conn.autocommit = True\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        # First get all dataset_identifiers matching the xml_filename\n",
        "        select_query = \"SELECT dataset_identifier FROM merged_dataset_metadata WHERE xml_filename = %s;\"\n",
        "        cur.execute(select_query, (xml_filename,))\n",
        "        dataset_ids = cur.fetchall()\n",
        "\n",
        "        if not dataset_ids:\n",
        "            print(f\"No entries found for xml_filename = '{xml_filename}'\")\n",
        "        else:\n",
        "            for dataset_id, in dataset_ids:\n",
        "                cur.execute(\"DELETE FROM merged_dataset_metadata WHERE dataset_identifier = %s;\", (dataset_id,))\n",
        "            print(f\"Deleted {len(dataset_ids)} dataset(s) and all related metadata entries for xml_filename = '{xml_filename}'\")\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error deleting metadata entries\", exception=e)\n",
        "\n",
        "# Example usage:\n",
        "# fetch_metadata_details(\"4M\", \"your_filename.xml\")\n",
        "delete_metadata_by_filename(\"4M\", \"zuri-wc.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No entries found for xml_filename = 'zuri-wc.xml'\n",
            "No entries found for xml_filename = '__.xml'\n",
            "No entries found for xml_filename = '__86-1.xml'\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "import json\n",
        "import csv\n",
        "\n",
        "config_file = r\"21_load\\\\db_config.json\"\n",
        "\n",
        "def load_config(config_path=config_file):\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def delete_metadata_by_filename(db_name, xml_filename):\n",
        "    \"\"\"\n",
        "    Delete entries from metadata tables by finding dataset_identifier(s) for the given xml_filename,\n",
        "    then deleting from the root dataset table, cascading to children via foreign keys.\n",
        "\n",
        "    Args:\n",
        "        db_name (str): PostgreSQL database name.\n",
        "        xml_filename (str): XML filename to match.\n",
        "    \"\"\"\n",
        "    config = load_config()\n",
        "\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=db_name,\n",
        "            user=config[\"user\"],\n",
        "            password=config[\"password\"],\n",
        "            host=config[\"host\"],\n",
        "            port=config[\"port\"]\n",
        "        )\n",
        "        conn.autocommit = True\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        # First get all dataset_identifiers matching the xml_filename\n",
        "        select_query = \"SELECT dataset_identifier FROM merged_dataset_metadata WHERE xml_filename = %s;\"\n",
        "        cur.execute(select_query, (xml_filename,))\n",
        "        dataset_ids = cur.fetchall()\n",
        "\n",
        "        if not dataset_ids:\n",
        "            print(f\"No entries found for xml_filename = '{xml_filename}'\")\n",
        "        else:\n",
        "            for dataset_id, in dataset_ids:\n",
        "                cur.execute(\"DELETE FROM merged_dataset_metadata WHERE dataset_identifier = %s;\", (dataset_id,))\n",
        "            print(f\"Deleted {len(dataset_ids)} dataset(s) and all related metadata entries for xml_filename = '{xml_filename}'\")\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error deleting metadata entries\", exception=e)\n",
        "\n",
        "def delete_from_csv_list(csv_path, db_name):\n",
        "    \"\"\"\n",
        "    Iterate through a CSV file and delete each metadata entry by Dataset_Name (xml_filename).\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the CSV file.\n",
        "        db_name (str): PostgreSQL database name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                xml_filename = row.get('Dataset_Name')\n",
        "                if xml_filename:\n",
        "                    delete_metadata_by_filename(db_name, xml_filename)\n",
        "    except Exception as e:\n",
        "        print(\"Error processing CSV file for deletions\", exception=e)\n",
        "\n",
        "# Example usage:\n",
        "delete_from_csv_list(\"removeorder_metadata_opendata.swiss.csv\", \"4M\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
