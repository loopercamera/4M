{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merging completed. 'merged_dataset_metadata.csv' saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\QGIS\\ipykernel_16340\\4024930570.py:8: DtypeWarning: Columns (12,14,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merging completed. 'merged_distribution_metadata.csv' saved successfully!\n",
      "✅ Merging completed. 'merged_contact_metadata.csv' saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(file1, file2, output_file):\n",
    "    \"\"\"\n",
    "    Merge two CSV files, keeping all columns from both files.\n",
    "    \"\"\"\n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Merge the dataframes, keeping all columns\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True, sort=False)  # Keep all columns, avoid sorting\n",
    "\n",
    "    # Save the merged dataframe as a CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"✅ Merging completed. '{output_file}' saved successfully!\")\n",
    "\n",
    "# Merge dataset metadata\n",
    "merge_csv_files('../01_opendata.swiss/opendata_datasets_metadata.csv', \n",
    "                '../02_geocat.ch/geocat_dataset_metadata.csv', \n",
    "                'merged_dataset_metadata.csv')\n",
    "\n",
    "# Merge distribution metadata\n",
    "merge_csv_files('../01_opendata.swiss/opendata_distribution_metadata.csv', \n",
    "                '../02_geocat.ch/geocat_distribution_metadata.csv', \n",
    "                'merged_distribution_metadata.csv')\n",
    "\n",
    "# Merge contact metadata\n",
    "merge_csv_files('../01_opendata.swiss/opendata_contact_metadata.csv', \n",
    "                '../02_geocat.ch/geocat_contact_metadata.csv', \n",
    "                'merged_contact_metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\QGIS\\ipykernel_16340\\4178093828.py:55: DtypeWarning: Columns (2,4,6,7,8,9,10,11,12,13,14,16,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_FILE)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(INPUT_FILE)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Run duplicate detection and removal\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m df_cleaned, df_removed \u001b[38;5;241m=\u001b[39m \u001b[43mfind_and_remove_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Save cleaned dataset (without duplicates)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m df_cleaned\u001b[38;5;241m.\u001b[39mto_csv(OUTPUT_FILE_CLEANED, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mfind_and_remove_duplicates\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     30\u001b[0m id_sim \u001b[38;5;241m=\u001b[39m fuzz\u001b[38;5;241m.\u001b[39mratio(\u001b[38;5;28mstr\u001b[39m(row1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mstr\u001b[39m(row2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Check title similarity\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m title_sim \u001b[38;5;241m=\u001b[39m \u001b[43mfuzz\u001b[49m\u001b[38;5;241m.\u001b[39mratio(\u001b[38;5;28mstr\u001b[39m(row1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_title_DE\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mstr\u001b[39m(row2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_title_DE\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# If both ID and title are highly similar, mark one for removal\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m id_sim \u001b[38;5;241m>\u001b[39m SIMILARITY_THRESHOLD \u001b[38;5;129;01mand\u001b[39;00m title_sim \u001b[38;5;241m>\u001b[39m SIMILARITY_THRESHOLD:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz  # Faster alternative to fuzzywuzzy\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"merged_dataset_metadata.csv\"  # Change to your actual file path\n",
    "OUTPUT_FILE_CLEANED = \"cleaned_dataset_metadata.csv\"\n",
    "OUTPUT_FILE_REMOVED = \"removed_duplicates.csv\"\n",
    "\n",
    "# Similarity threshold (0-100): Higher means stricter matching\n",
    "SIMILARITY_THRESHOLD = 90  \n",
    "\n",
    "def find_and_remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Identify and remove similar records while keeping 'opendata.swiss' over 'geocat.ch'.\n",
    "    Returns both cleaned and removed data.\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(by=[\"origin\"], ascending=True)  # Ensures opendata.swiss comes first\n",
    "    to_remove = set()\n",
    "    removed_data = []\n",
    "\n",
    "    for i, row1 in df_sorted.iterrows():\n",
    "        if i in to_remove:  # Skip already marked duplicates\n",
    "            continue\n",
    "        for j, row2 in df_sorted.iterrows():\n",
    "            if i >= j or j in to_remove:  # Avoid redundant checks\n",
    "                continue\n",
    "\n",
    "            # Check same dataset identifier (ignoring minor differences like suffixes)\n",
    "            id_sim = fuzz.ratio(str(row1[\"dataset_identifier\"]), str(row2[\"dataset_identifier\"]))\n",
    "\n",
    "            # Check title similarity\n",
    "            title_sim = fuzz.ratio(str(row1[\"dataset_title_DE\"]), str(row2[\"dataset_title_DE\"]))\n",
    "\n",
    "            # If both ID and title are highly similar, mark one for removal\n",
    "            if id_sim > SIMILARITY_THRESHOLD and title_sim > SIMILARITY_THRESHOLD:\n",
    "                if row1[\"origin\"] == \"geocat.ch\":\n",
    "                    to_remove.add(i)\n",
    "                    removed_data.append(row1)  # Store removed data\n",
    "                else:\n",
    "                    to_remove.add(j)\n",
    "                    removed_data.append(row2)\n",
    "\n",
    "    # Create DataFrame of removed duplicates\n",
    "    df_removed = pd.DataFrame(removed_data)\n",
    "\n",
    "    # Remove marked duplicates from the original dataset\n",
    "    df_cleaned = df_sorted.drop(index=to_remove)\n",
    "\n",
    "    return df_cleaned, df_removed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure input file exists\n",
    "    if os.path.exists(INPUT_FILE):\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "        # Run duplicate detection and removal\n",
    "        df_cleaned, df_removed = find_and_remove_duplicates(df)\n",
    "\n",
    "        # Save cleaned dataset (without duplicates)\n",
    "        df_cleaned.to_csv(OUTPUT_FILE_CLEANED, index=False)\n",
    "\n",
    "        # Save removed duplicates\n",
    "        df_removed.to_csv(OUTPUT_FILE_REMOVED, index=False)\n",
    "\n",
    "        print(f\"✅ Cleaned dataset saved as: {OUTPUT_FILE_CLEANED}\")\n",
    "        print(f\"✅ Removed duplicates saved as: {OUTPUT_FILE_REMOVED}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: Input file '{INPUT_FILE}' not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
