{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "\n",
    "def sort_xml(elem):\n",
    "    \"\"\"Sorts the children of an XML element by tag name and attributes.\"\"\"\n",
    "    if len(elem) == 0:\n",
    "        return\n",
    "\n",
    "    elem[:] = sorted(elem, key=lambda e: (e.tag, sorted(e.attrib.items())))\n",
    "\n",
    "    for child in elem:\n",
    "        sort_xml(child)\n",
    "\n",
    "def prettify_xml(elem):\n",
    "    \"\"\"Return a pretty-printed XML string.\"\"\"\n",
    "    rough_string = ET.tostring(elem, encoding='utf-8')\n",
    "    parsed = minidom.parseString(rough_string)\n",
    "    return parsed.toprettyxml(indent=\"  \")\n",
    "\n",
    "# Load and parse the XML\n",
    "tree = ET.parse(\"01_opendata.swiss/test/__.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Sort recursively\n",
    "sort_xml(root)\n",
    "\n",
    "# Save pretty sorted XML\n",
    "with open(\"file1_sorted.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(prettify_xml(root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"01_opendata.swiss/test\"  # Replace with the actual folder path\n",
    "\n",
    "dataset_names = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        dataset_names.append(filename[:-4])  # Remove the file extension\n",
    "\n",
    "df = pd.DataFrame({\"Dataset_Name\": dataset_names})\n",
    "df.to_csv(\"01_opendata.swiss/dataset_names.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset_Name                                       dataset_hash\n",
      "0     test.xml  e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b93...\n",
      "Hashes calculated and saved to dataset_names.csv\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the folder path\n",
    "folder_path = \"01_opendata.swiss/test\"  # Change this to the correct folder\n",
    "\n",
    "# Function to compute SHA-256 hash of a file\n",
    "def file_hash(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "# List all XML files in the folder and compute their hashes\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith(\".xml\")]\n",
    "df = pd.DataFrame({\"Dataset_Name\": files, \"dataset_hash\": [file_hash(os.path.join(folder_path, f)) for f in files]})\n",
    "\n",
    "# Save the hashes to a CSV file\n",
    "df.to_csv(\"01_opendata.swiss/NHF.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "print(\"Hashes calculated and saved to dataset_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset_Name                                       dataset_hash status\n",
      "0     test.xml  e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b93...    new\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "nhf_file = \"01_opendata.swiss/NHF.csv\"\n",
    "ohf_file = \"01_opendata.swiss/OHF.csv\"\n",
    "\n",
    "# Load NHF file\n",
    "nhf_df = pd.read_csv(nhf_file)\n",
    "\n",
    "# Check if OHF file exists\n",
    "if os.path.exists(ohf_file):\n",
    "    ohf_df = pd.read_csv(ohf_file)\n",
    "    \n",
    "    # Merge NHF with OHF to track all entries\n",
    "    comparison_df = pd.merge(nhf_df, ohf_df, on=\"Dataset_Name\", how=\"left\", suffixes=('_NHF', '_OHF'))\n",
    "\n",
    "    # Define status column for NHF-based comparison\n",
    "    def get_status(row):\n",
    "        if pd.isna(row[\"dataset_hash_OHF\"]):  # Not found in OHF\n",
    "            return \"new\"\n",
    "        elif row[\"dataset_hash_NHF\"] == row[\"dataset_hash_OHF\"]:  # Exact match\n",
    "            return \"found\"\n",
    "        else:  # Hash is different\n",
    "            return \"changed\"\n",
    "\n",
    "    comparison_df[\"status\"] = comparison_df.apply(get_status, axis=1)\n",
    "\n",
    "    # Rename column for consistency\n",
    "    comparison_df.rename(columns={\"dataset_hash_NHF\": \"dataset_hash\"}, inplace=True)\n",
    "\n",
    "    # Drop the OHF hash column\n",
    "    comparison_df = comparison_df.drop(columns=[\"dataset_hash_OHF\"])\n",
    "\n",
    "    # Identify entries in OHF that are missing in NHF (i.e., removed)\n",
    "    removed_df = ohf_df[~ohf_df[\"Dataset_Name\"].isin(nhf_df[\"Dataset_Name\"])].copy()\n",
    "    removed_df.rename(columns={\"dataset_hash\": \"dataset_hash\"}, inplace=True)\n",
    "    removed_df[\"status\"] = \"removed\"\n",
    "\n",
    "    # Select relevant columns for removed_df\n",
    "    removed_df = removed_df[[\"Dataset_Name\", \"dataset_hash\", \"status\"]]\n",
    "\n",
    "    # Combine both dataframes\n",
    "    final_df = pd.concat([comparison_df, removed_df], ignore_index=True)\n",
    "else:\n",
    "    # If OHF does not exist, mark all datasets as new\n",
    "    nhf_df[\"status\"] = \"new\"\n",
    "    final_df = nhf_df\n",
    "\n",
    "# Display or save the output\n",
    "print(final_df)\n",
    "final_df.to_csv(\"01_opendata.swiss/comparison_result.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the comparison result\n",
    "comparison_file = \"01_opendata.swiss/comparison_result.csv\"\n",
    "df = pd.read_csv(comparison_file)\n",
    "\n",
    "# Folder containing the files\n",
    "folder_path = \"01_opendata.swiss/test\"\n",
    "\n",
    "# Get the list of files to remove\n",
    "files_to_remove = df[df[\"status\"] == \"found\"][\"Dataset_Name\"].tolist()\n",
    "\n",
    "# Remove the files\n",
    "for file_name in files_to_remove:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {file_name}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "\n",
    "print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove order metadata saved to 01_opendata.swiss/removeorder_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the comparison result\n",
    "comparison_file = \"01_opendata.swiss/comparison_result.csv\"\n",
    "df = pd.read_csv(comparison_file)\n",
    "\n",
    "# Filter for files with status \"changed\" or \"removed\"\n",
    "remove_order_df = df[df[\"status\"].isin([\"changed\", \"removed\"])]\n",
    "\n",
    "# Save to new CSV file\n",
    "remove_order_file = \"01_opendata.swiss/removeorder_metadata.csv\"\n",
    "remove_order_df.to_csv(remove_order_file, index=False)\n",
    "\n",
    "print(f\"Remove order metadata saved to {remove_order_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHF.csv has been updated with relevant dataset entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the comparison result\n",
    "comparison_file = \"01_opendata.swiss/comparison_result.csv\"\n",
    "df = pd.read_csv(comparison_file)\n",
    "\n",
    "# Filter for statuses: found, changed, and new\n",
    "valid_statuses = [\"found\", \"changed\", \"new\"]\n",
    "filtered_df = df[df[\"status\"].isin(valid_statuses)][[\"Dataset_Name\", \"dataset_hash\"]]\n",
    "\n",
    "# Save the filtered data to OHF.csv (override it)\n",
    "oh_file = \"01_opendata.swiss/OHF.csv\"\n",
    "filtered_df.to_csv(oh_file, index=False)\n",
    "\n",
    "print(f\"OHF.csv has been updated with relevant dataset entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Dataset_Name  \\\n",
      "0             10-ag-bieneninspektionskreise   \n",
      "1                         11-ag-schulkreise   \n",
      "2  116-ch-kataster-der-belasteten-standorte   \n",
      "3                                        __   \n",
      "4                                     __101   \n",
      "\n",
      "                                        dataset_hash status  \n",
      "0  c50317be73339eecb370cf489b4379b79b6e8675e85031...  found  \n",
      "1  6d5f3b6d1fbc67cbd9d19d7fb9802a98376e7e5b284693...  found  \n",
      "2  85be8ac70f3fd7158b3a86e71b3e668f86f6011bd44e34...  found  \n",
      "3  c4c3e4b2e37f13dfbec088dcd36841a3e7e6db3d3fa716...  found  \n",
      "4  2178eb1e48cde5c0b9c0a24689203e4b72be9768404563...  found  \n",
      "Hashes calculated and saved to comparison_result.csv\n",
      "Removed: 10-ag-bieneninspektionskreise.xml\n",
      "Removed: 11-ag-schulkreise.xml\n",
      "Removed: 116-ch-kataster-der-belasteten-standorte.xml\n",
      "Removed: __.xml\n",
      "Removed: __101.xml\n",
      "Cleanup complete.\n",
      "Remove order metadata saved to removeorder_metadata.csv\n",
      "previous_import.csv has been updated with relevant dataset entries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"01_opendata.swiss/test\"  # Replace with the actual folder path\n",
    "latest_import = \"01_opendata.swiss/latest_import.csv\"\n",
    "previous_import = \"01_opendata.swiss/previous_import.csv\"\n",
    "comparison_file = \"01_opendata.swiss/comparison_result.csv\"\n",
    "remove_order_file = \"removeorder_metadata.csv\"\n",
    "\n",
    "# Function to compute SHA-256 hash of a file\n",
    "def file_hash(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "# List all XML files in the folder and compute their names and hashes\n",
    "dataset_names = []\n",
    "dataset_hashes = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        dataset_names.append(filename[:-4])  # Remove the file extension\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        dataset_hashes.append(file_hash(file_path))\n",
    "\n",
    "# Save latest import\n",
    "latest_df = pd.DataFrame({\"Dataset_Name\": dataset_names, \"dataset_hash\": dataset_hashes})\n",
    "latest_df.to_csv(latest_import, index=False)\n",
    "\n",
    "# Compare latest with previous if previous exists\n",
    "if os.path.exists(previous_import):\n",
    "    previous_df = pd.read_csv(previous_import)\n",
    "\n",
    "    # Ensure consistent types for merge\n",
    "    latest_df[\"Dataset_Name\"] = latest_df[\"Dataset_Name\"].astype(str)\n",
    "    previous_df[\"Dataset_Name\"] = previous_df[\"Dataset_Name\"].astype(str)\n",
    "\n",
    "    if not latest_df.empty:\n",
    "        # Merge to track all entries\n",
    "        comparison_df = pd.merge(latest_df, previous_df, on=\"Dataset_Name\", how=\"left\", suffixes=('_latest', '_previous'))\n",
    "\n",
    "        # Define status column for comparison\n",
    "        def get_status(row):\n",
    "            if pd.isna(row[\"dataset_hash_previous\"]):\n",
    "                return \"new\"\n",
    "            elif row[\"dataset_hash_latest\"] == row[\"dataset_hash_previous\"]:\n",
    "                return \"found\"\n",
    "            else:\n",
    "                return \"changed\"\n",
    "\n",
    "        comparison_df[\"status\"] = comparison_df.apply(get_status, axis=1)\n",
    "\n",
    "        # Rename and clean columns\n",
    "        comparison_df.rename(columns={\"dataset_hash_latest\": \"dataset_hash\"}, inplace=True)\n",
    "        comparison_df = comparison_df.drop(columns=[\"dataset_hash_previous\"])\n",
    "    else:\n",
    "        comparison_df = pd.DataFrame(columns=[\"Dataset_Name\", \"dataset_hash\", \"status\"])\n",
    "\n",
    "    # Identify entries in previous import missing in latest (i.e., removed)\n",
    "    removed_df = previous_df[~previous_df[\"Dataset_Name\"].isin(latest_df[\"Dataset_Name\"])]\n",
    "    removed_df[\"status\"] = \"removed\"\n",
    "\n",
    "    # Combine current and removed datasets\n",
    "    final_df = pd.concat([comparison_df, removed_df], ignore_index=True)\n",
    "else:\n",
    "    latest_df[\"status\"] = \"new\"\n",
    "    final_df = latest_df\n",
    "\n",
    "# Save comparison result\n",
    "final_df.to_csv(comparison_file, index=False)\n",
    "\n",
    "# Display sample\n",
    "print(final_df.head())\n",
    "print(\"Hashes calculated and saved to comparison_result.csv\")\n",
    "\n",
    "# Cleanup step: remove files marked as 'found'\n",
    "df = pd.read_csv(comparison_file)\n",
    "files_to_remove = df[df[\"status\"] == \"found\"][\"Dataset_Name\"].tolist()\n",
    "\n",
    "for file_name in files_to_remove:\n",
    "    file_path = os.path.join(folder_path, file_name + \".xml\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {file_name}.xml\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}.xml\")\n",
    "\n",
    "print(\"Cleanup complete.\")\n",
    "\n",
    "# Save metadata for files marked as 'changed' or 'removed'\n",
    "remove_order_df = df[df[\"status\"].isin([\"changed\", \"removed\"])]\n",
    "remove_order_df.to_csv(remove_order_file, index=False)\n",
    "print(f\"Remove order metadata saved to {remove_order_file}\")\n",
    "\n",
    "# Update previous import with found, changed, and new entries\n",
    "valid_statuses = [\"found\", \"changed\", \"new\"]\n",
    "filtered_df = df[df[\"status\"].isin(valid_statuses)][[\"Dataset_Name\", \"dataset_hash\"]]\n",
    "filtered_df.to_csv(previous_import, index=False)\n",
    "print(\"previous_import.csv has been updated with relevant dataset entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Dataset_Name, dataset_hash, status]\n",
      "Index: []\n",
      "Hashes calculated and saved to comparison_result.csv\n",
      "Cleanup complete.\n",
      "Remove order metadata saved to removeorder_metadata.csv\n",
      "previous_import.csv has been updated with relevant dataset entries.\n"
     ]
    }
   ],
   "source": [
    "from dataset_change_detector import compare_dataset_hashes\n",
    "\n",
    "compare_dataset_hashes(\"01_opendata.swiss\", \"removeorder_metadata_opendata.swiss.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
