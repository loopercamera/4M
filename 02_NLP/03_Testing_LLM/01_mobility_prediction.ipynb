{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1f3115",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook processes all labeled entries by passing them through a prompt to the large language model (LLM) Google Gemini. It handles the data in batches of 10 rows from the DataFrame and waits for the result after each batch.\n",
    "\n",
    "#### Improvments to consider\n",
    "- Create multiple API keys to allow more requests per minute and per day.\n",
    "- The prompt is somewhat unstable, as multiple entries are processed in a single request. Processing one entry at a time with max_output_tokens = 1 would be much more stable.\n",
    "- Define promps in other languages\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f963e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import time\n",
    "\n",
    "\n",
    "inputdata_file = '../02_Prototype_LM_BERT/data/03_labelled_data.csv' #currently the same file as in 02_Prototype_LM_BERT\n",
    "outputdata_file ='data/02_predicted_data.csv'\n",
    "\n",
    "with open(\"data/apikeys.json\") as f:\n",
    "    config = json.load(f)\n",
    "API_KEYS = config[\"GOOGLE_API_KEYS\"]\n",
    "API_KEYS_CYCLE = cycle(API_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a551bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labelled rows after filtering: 150\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(inputdata_file, dtype={'mobilitydata_labelled': 'string'}, low_memory=False)\n",
    "\n",
    "# Drop rows where 'mobilitydata_labelled' is empty (NaN)\n",
    "df = df.dropna(subset=['mobilitydata_labelled'])\n",
    "\n",
    "# Convert 'mobilitydata_labelled' to boolean type\n",
    "df['mobilitydata_labelled'] = df['mobilitydata_labelled'].map({'True': True, 'False': False})\n",
    "\n",
    "# Print the number of rows remaining after filtering\n",
    "print(f\"Number of labelled rows after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "requests_per_key = 15  # max requests per key per minute\n",
    "key_count = len(API_KEYS)\n",
    "current_key_index = 0\n",
    "key_request_counter = 0\n",
    "cycle_start_time = time.time()\n",
    "\n",
    "\n",
    "def process_chunks(df, indices, chunk_size):\n",
    "    global current_key_index, key_request_counter, cycle_start_time\n",
    "\n",
    "    for i in tqdm(range(0, len(indices), chunk_size)):\n",
    "        # Rotate key if limit reached\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Max requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        # Prepare data\n",
    "        batch_indices = indices[i:i + chunk_size]\n",
    "        chunk_df = df.loc[batch_indices][['dataset_title_DE', 'dataset_description_DE']]\n",
    "\n",
    "        chunk_lines = chunk_df.apply(\n",
    "            lambda row: f\"Titel: {row['dataset_title_DE']}\\nBeschreibung: {row['dataset_description_DE']}\",\n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        # prompt = (\n",
    "        #     \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\\n\\n\" +\n",
    "        #     \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "        #     \"Antworte nur mit T oder F für True und False und reihe alle Antworten direkt aneinander. \"\n",
    "        #     \"Ohne Leerzeichen, Umbrüche, Texte, Sonderzeichen.\"\n",
    "        # )\n",
    "\n",
    "        # prompt = (\n",
    "        #     \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\\n\\n\" +\n",
    "        #     \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "        #     \"Antworte nur mit T oder F für True und False und reihe alle Antworten direkt aneinander ohne Leerzeichen, Umbrüche, Texte oder Sonderzeichen zu verwenden. Beispielhaftes Antwortschema:\\n\\n\" +\n",
    "        #     \"TFTFFTFTTT\"\n",
    "        # )\n",
    "\n",
    "        # prompt = (\n",
    "        #     \"Untenstehend finden Sie jeweils einen Titel und eine Beschreibung eines Datensatzes. \"\n",
    "        #     \"Bewerten Sie für jede Kombination, ob es sich um Verkehrs- oder Mobilitätsdaten handelt.\\n\\n\"\n",
    "        #     \"Antworten Sie für jede Zeile **nur** mit:\\n\"\n",
    "        #     \"- **T** für True = es handelt sich um Mobilitätsdaten\\n\"\n",
    "        #     \"- **F** für False = es handelt sich **nicht** um Mobilitätsdaten\\n\\n\"\n",
    "        #     \"Reihen Sie alle Antworten ohne Leerzeichen, Umbrüche oder andere Zeichen direkt hintereinander. \"\n",
    "        #     \"Geben Sie ausschließlich eine Zeichenkette bestehend aus T und F zurück.\\n\\n\"\n",
    "        #     \"Beispiel für 10 Einträge:\\n\"\n",
    "        #     \"TFTFTFTFFT\\n\\n\"\n",
    "        #     \"Datensätze:\\n\\n\" +\n",
    "        #     \"\\n\\n\".join(chunk_lines)\n",
    "        # )\n",
    "\n",
    "        # prompt = (\n",
    "        #     \"Entscheide für jeden der folgenden Datensätze, ob es sich um Verkehrs- oder Mobilitätsdaten handelt.\\n\\n\" +\n",
    "        #     \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "        #     \"Antworte nur mit T für 'True' (Mobilitätsdaten) oder F für 'False' (keine Mobilitätsdaten).\\n\"\n",
    "        #     \"Gib ausschließlich eine Zeichenkette aus T und F zurück – **ohne Leerzeichen, Umbrüche oder andere Zeichen**.\\n\\n\"\n",
    "        #     \"Beispielantwort für zehn Datensätze:\\nTFTFFTFTTT\"\n",
    "        # )\n",
    "\n",
    "        # prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten? Antworte nur mit T (True) oder F (False) Zeilenweise.\\n\\n\" + \"\\n\\n\".join(chunk_lines)\n",
    "        \n",
    "        prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten? Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines)\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.models.generate_content_stream(\n",
    "                    model=\"gemini-2.0-flash-lite-001\",\n",
    "                    contents=[prompt],\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        max_output_tokens=chunk_size,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                )\n",
    "                result_text = \"\".join(chunk.text for chunk in response)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"Rate limit reached. Waiting 60 seconds... (Attempt {attempt+1} of {max_retries})\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: {error_str}\")\n",
    "                    break\n",
    "        else:\n",
    "            df.loc[batch_indices, 'mobilitydata_generated'] = \"ERROR\"\n",
    "            continue\n",
    "\n",
    "        # Process model output\n",
    "        predictions = list(result_text.strip())\n",
    "\n",
    "        if all(p in ['T', 'F'] for p in predictions) and len(predictions) == len(batch_indices):\n",
    "            df.loc[batch_indices, 'mobilitydata_generated'] = predictions\n",
    "        else:\n",
    "            df.loc[batch_indices, 'mobilitydata_generated'] = \"ERROR\"\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)\n",
    "\n",
    "# Main logic\n",
    "df['mobilitydata_generated'] = None\n",
    "\n",
    "# First run with chunk_size = 10\n",
    "all_indices = df.index.tolist()\n",
    "process_chunks(df, all_indices, chunk_size=10)\n",
    "\n",
    "# Retry failed ones with chunk_size = 5\n",
    "retry_indices = df[df['mobilitydata_generated'] == \"ERROR\"].index.tolist()\n",
    "process_chunks(df, retry_indices, chunk_size=5)\n",
    "\n",
    "# Final attempt with chunk_size = 1\n",
    "retry_indices = df[df['mobilitydata_generated'] == \"ERROR\"].index.tolist()\n",
    "process_chunks(df, retry_indices, chunk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846794c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 1 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 1 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:38<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 1 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:59<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 2 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 2 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 2 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:57<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 3 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 3 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:37<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 3 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:56<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 4 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:20<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 4 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 4 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:57<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 5 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 5 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:37<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 5 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:56<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 6 with chunk_size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 6 with chunk_size 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 6 with chunk_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:56<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "requests_per_key = 15\n",
    "key_count = len(API_KEYS)\n",
    "current_key_index = 0\n",
    "key_request_counter = 0\n",
    "cycle_start_time = time.time()\n",
    "\n",
    "def build_prompt(prompt_id, chunk_lines):\n",
    "    if prompt_id == 1:\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\\n\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "            \"Antworte nur mit T für True, F für False oder U für Uncertain und reihe alle Antworten direkt aneinander. \"\n",
    "            \"Ohne Leerzeichen, Umbrüche, Texte, Sonderzeichen.\"\n",
    "        )\n",
    "    elif prompt_id == 2:\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\\n\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "            \"Antworte nur mit T für True, F für False oder U für Uncertain und reihe alle Antworten direkt aneinander ohne Leerzeichen, Umbrüche, Texte oder Sonderzeichen zu verwenden. Beispielhaftes Antwortschema:\\n\\n\" +\n",
    "            \"TFTFFTFUTT\"\n",
    "        )\n",
    "    elif prompt_id == 3:\n",
    "        return (\n",
    "            \"Untenstehend finden Sie jeweils einen Titel und eine Beschreibung eines Datensatzes.\"\n",
    "            \"Bewerten Sie für jede Kombination, ob es sich um Verkehrs- oder Mobilitätsdaten handelt.\\n\\n\"\n",
    "            \"Antworten Sie für jede Zeile **nur** mit:\\n\"\n",
    "            \"- **T** für True = es handelt sich um Mobilitätsdaten\\n\"\n",
    "            \"- **F** für False = es handelt sich **nicht** um Mobilitätsdaten\\n\"\n",
    "            \"- **U** für Uncertain = Sie sind sich unsicher in der Zuweisung\\n\\n\"\n",
    "            \"Reihen Sie alle Antworten ohne Leerzeichen, Umbrüche oder andere Zeichen direkt hintereinander. \"\n",
    "            \"Geben Sie ausschließlich eine Zeichenkette bestehend aus T, F und U zurück.\\n\\n\"\n",
    "            \"Beispiel für 10 Einträge:\\n\"\n",
    "            \"TFTFTFTUFT\\n\\n\"\n",
    "            \"Datensätze:\\n\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines)\n",
    "        )\n",
    "    elif prompt_id == 4:\n",
    "        return (\n",
    "            \"Als Experte für Datenannotation im Bereich Mobilitäts- und Verkehrsdaten ist es Ihre Aufgabe, öffentliche Datensätze daraufhin zu prüfen, ob sie Informationen enthalten, die eindeutig Mobilitäts- oder Verkehrsdaten betreffen. Ihre Einschätzung hilft bei der sachgerechten Klassifizierung dieser Inhalte auf einem nationalen Datenportal.\\n\"\n",
    "            \"Aufgabe:\\n\" \n",
    "            \"Beurteilen Sie, ob es sich bei dem folgenden Datensatz um Mobilitäts- oder Verkehrsdaten handelt.\\n\\n\"\n",
    "            \"Antwortformat:\\n\"  \n",
    "            \"Antworten Sie **nur mit einer der folgenden Optionen**, ohne zusätzliche Zeichen oder Erläuterungen (T für True, F für False, U für Uncertain):\\n\\n\"\n",
    "            \"- T\\n\"  \n",
    "            \"- F\\n\"  \n",
    "            \"- U\\n\\n\"\n",
    "            \"Hinweise:\\n\" \n",
    "            \"- Verwenden Sie **U: Uncertain**, wenn die Informationen im Titel oder in der Beschreibung unklar oder nicht ausreichend sind.\\n\"  \n",
    "            \"- Berücksichtigen Sie Aspekte wie Verkehrsmittel, Infrastruktur, Mobilitätsverhalten oder Verkehrsfluss.\\n\"  \n",
    "            \"- Geben Sie keine zusätzlichen Erläuterungen – nur die ausgewählte Option.\\n\\n\"\n",
    "            \"Datensatzbeschreibung:\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines) +\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "    elif prompt_id == 5:\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten? \"\n",
    "            \"Antworte nur mit T (True), F (False) oder U (Uncertain).\\n\\n\" + \"\\n\\n\".join(chunk_lines)\n",
    "        )\n",
    "    elif prompt_id == 6:\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten? \"\n",
    "            \"Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Ungültige Prompt-ID\")\n",
    "\n",
    "\n",
    "def process_chunks(df, indices, chunk_size, prompt_id):\n",
    "    global current_key_index, key_request_counter, cycle_start_time\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(indices), chunk_size)):\n",
    "        # Rotate key if needed\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Max requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        batch_indices = indices[i:i + chunk_size]\n",
    "        chunk_df = df.loc[batch_indices][['dataset_title_DE', 'dataset_description_DE']]\n",
    "\n",
    "        chunk_lines = chunk_df.apply(\n",
    "            lambda row: f\"Titel: {row['dataset_title_DE']}\\nBeschreibung: {row['dataset_description_DE']}\",\n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        prompt = build_prompt(prompt_id, chunk_lines)\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.models.generate_content_stream(\n",
    "                    model=\"gemini-2.0-flash-lite-001\",\n",
    "                    contents=[prompt],\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        max_output_tokens=chunk_size,\n",
    "                        temperature=0\n",
    "                        top_p=0.95, # Defaultvalue\n",
    "                        top_k=64, # Defaultvalue\n",
    "                        candidate_count=1, # Defaultvalue\n",
    "                    )\n",
    "                )\n",
    "                result_text = \"\".join(chunk.text for chunk in response)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"Rate limit reached. Waiting 60 seconds... (Attempt {attempt+1} of {max_retries})\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: {error_str}\")\n",
    "                    break\n",
    "        else:\n",
    "            # All retries failed\n",
    "            for idx in batch_indices:\n",
    "                results.append({\n",
    "                    \"index\": idx,\n",
    "                    \"mobilitydata_generated\": \"ERROR\",\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"chunk_size\": chunk_size\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        # Split result\n",
    "        predictions = list(result_text.strip())\n",
    "        for rel_idx, prediction in zip(batch_indices, predictions):\n",
    "            results.append({\n",
    "                \"index\": rel_idx,\n",
    "                \"mobilitydata_generated\": prediction if prediction in [\"T\", \"F\", \"U\"] else \"ERROR\",\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"chunk_size\": chunk_size\n",
    "            })\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ----- Main Evaluation -----\n",
    "\n",
    "# Baseline DataFrame vorbereiten\n",
    "df['mobilitydata_generated'] = None\n",
    "all_indices = df.index.tolist()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Lege neue Spalten für alle Prompt-/Chunk-Kombinationen an\n",
    "for prompt_id in range(1, 7):\n",
    "    for chunk_size in [10, 5, 1]:\n",
    "        col_name = f\"mobilitydata_generated_p{prompt_id}_c{chunk_size}\"\n",
    "        df[col_name] = None\n",
    "\n",
    "# Führe Modellabfragen für alle Kombinationen aus und sammle Ergebnisse\n",
    "for prompt_id in range(1, 7):\n",
    "    for chunk_size in [10, 5, 1]:\n",
    "        print(f\"Running Prompt {prompt_id} with chunk_size {chunk_size}\")\n",
    "        result_rows = process_chunks(df.copy(), all_indices, chunk_size, prompt_id)\n",
    "        all_results.extend(result_rows)\n",
    "\n",
    "# Ergebnisse in die jeweiligen Spalten schreiben\n",
    "for row in all_results:\n",
    "    idx = row['index']\n",
    "    prompt_id = row['prompt_id']\n",
    "    chunk_size = row['chunk_size']\n",
    "    value = row['mobilitydata_generated']\n",
    "    col_name = f\"mobilitydata_generated_p{prompt_id}_c{chunk_size}\"\n",
    "    df.at[idx, col_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a804b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auswertung der Prompt-/Chunk-Kombinationen:\n",
      " prompt_id  chunk_size  TP  TN  FP  FN  Total  Accuracy  Precision  Recall  F1-Score\n",
      "         1           1  25 118   3   4    150    0.9533     0.8929  0.8621    0.8772\n",
      "         1           5  25  72  47   3    147    0.6599     0.3472  0.8929    0.5000\n",
      "         1          10  18  57  47   7    129    0.5814     0.2769  0.7200    0.4000\n",
      "         2           1  25 115   6   4    150    0.9333     0.8065  0.8621    0.8333\n",
      "         2           5  25  82  38   4    149    0.7181     0.3968  0.8621    0.5435\n",
      "         2          10  21  65  42   7    135    0.6370     0.3333  0.7500    0.4615\n",
      "         3           1  24 116   5   5    150    0.9333     0.8276  0.8276    0.8276\n",
      "         3           5  21  80  37   8    146    0.6918     0.3621  0.7241    0.4828\n",
      "         3          10  19  76  40  10    145    0.6552     0.3220  0.6552    0.4318\n",
      "         4           1  19 119   2  10    150    0.9200     0.9048  0.6552    0.7600\n",
      "         4           5  18  94  26  11    149    0.7517     0.4091  0.6207    0.4932\n",
      "         4          10  15  84  35  14    148    0.6689     0.3000  0.5172    0.3797\n",
      "         5           1  26 106  15   3    150    0.8800     0.6341  0.8966    0.7429\n",
      "         5           5  10  58  12   7     87    0.7816     0.4545  0.5882    0.5128\n",
      "         5          10   4  37  12   8     61    0.6721     0.2500  0.3333    0.2857\n",
      "         6           1  25 118   3   4    150    0.9533     0.8929  0.8621    0.8772\n",
      "         6           5   7  53   8   7     75    0.8000     0.4667  0.5000    0.4828\n",
      "         6          10   0   8   1   1     10    0.8000     0.0000  0.0000    0.0000\n"
     ]
    }
   ],
   "source": [
    "analysis = []\n",
    "\n",
    "# Ergebnis-Spalten automatisch erkennen\n",
    "result_columns = [col for col in df.columns if col.startswith(\"mobilitydata_generated_p\")]\n",
    "\n",
    "for col in sorted(result_columns):\n",
    "    try:\n",
    "        parts = col.split(\"_\")\n",
    "        prompt_id = int(parts[2][1:])   # korrekt: 'p1'\n",
    "        chunk_size = int(parts[3][1:])  # korrekt: 'c10'\n",
    "\n",
    "        valid = df[df[col].isin(['T', 'F'])].copy()\n",
    "        valid['prediction'] = valid[col].map({'T': True, 'F': False})\n",
    "\n",
    "        tp = ((valid['mobilitydata_labelled'] == True) & (valid['prediction'] == True)).sum()\n",
    "        tn = ((valid['mobilitydata_labelled'] == False) & (valid['prediction'] == False)).sum()\n",
    "        fp = ((valid['mobilitydata_labelled'] == False) & (valid['prediction'] == True)).sum()\n",
    "        fn = ((valid['mobilitydata_labelled'] == True) & (valid['prediction'] == False)).sum()\n",
    "        total = len(valid)\n",
    "\n",
    "        accuracy = (tp + tn) / total if total > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        analysis.append({\n",
    "            'prompt_id': prompt_id,\n",
    "            'chunk_size': chunk_size,\n",
    "            'TP': tp,\n",
    "            'TN': tn,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Total': total,\n",
    "            'Accuracy': round(accuracy, 4),\n",
    "            'Precision': round(precision, 4),\n",
    "            'Recall': round(recall, 4),\n",
    "            'F1-Score': round(f1_score, 4)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Überspringe Spalte {col} wegen Fehler: {e}\")\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis)\n",
    "analysis_df = analysis_df.sort_values(['prompt_id', 'chunk_size'])\n",
    "\n",
    "print(\"\\nAuswertung der Prompt-/Chunk-Kombinationen:\")\n",
    "print(analysis_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2793e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been successfully saved as data/02_predicted_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Write dataframe in new csv-File\n",
    "df.to_csv(outputdata_file, index=False)\n",
    "\n",
    "print(f'The file has been successfully saved as {outputdata_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05129204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
