{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this Notebook a LLM from BERT is used to see, if and how it can be finetuned using own keywords. \n",
    "\n",
    "#### Result\n",
    "No difference found between the model before and after the training, BERT seems not to be what we search for. For Word embeddings and word2vec in german may exist better libraries/models. But also our knowledge about finetuning LLMs is not that deep. Training an own LLM could be a better choice, but is not suitable for our work. May try different approach using BERT or other libraries.\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-0.2487,  0.1334]])\n",
      "Vorhersage: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n",
    "\n",
    "# Example text\n",
    "text = \"Verkehrsdaten in Berlin\"\n",
    "\n",
    "# Tokenize text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Make prediction (unusually good due to the untrained classification layer)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Results (Logits)\n",
    "logits = outputs.logits\n",
    "print(f\"Logits: {logits}\")\n",
    "\n",
    "# Prediction (Index of the highest probability)\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "print(f\"Prediction: {prediction.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for mobility terms\n",
    "mobilitaetsbegriffe = [\n",
    "    \"Verkehr\", \"Transport\", \"Fahrzeug\", \"Straße\", \"Fahrplan\", \"Auto\", \"Bahn\", \"Mobilität\",\n",
    "    \"Fahrrad\", \"Bus\", \"E-Mobilität\", \"ÖPNV\", \"Flughafen\", \"Lkw\", \"Velo\", \"Fabian\", \"XYZ\", \"Essen\", \"Mami\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    # Tokenize and convert to tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # We take the [CLS] token as the representation of the text\n",
    "    # outputs.last_hidden_state is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "    # We take the first token ([CLS] token) and extract the embedding\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Example text\n",
    "text = \"Reiseverhalten\"\n",
    "\n",
    "# Calculate the embedding of the example text\n",
    "text_embedding = get_embedding(text)\n",
    "\n",
    "# Calculate embeddings of the mobility terms\n",
    "mobilitaets_embeddings = [get_embedding(term) for term in mobilitaetsbegriffe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilität: 0.8375\n",
      "E-Mobilität: 0.8211\n",
      "Verkehr: 0.7885\n",
      "Auto: 0.7408\n",
      "Fahrplan: 0.7294\n",
      "Velo: 0.7192\n",
      "Fahrrad: 0.7171\n",
      "Transport: 0.7162\n",
      "Lkw: 0.7160\n",
      "Essen: 0.7158\n",
      "Fahrzeug: 0.6914\n",
      "ÖPNV: 0.6898\n",
      "Bahn: 0.6739\n",
      "XYZ: 0.6719\n",
      "Bus: 0.6578\n",
      "Flughafen: 0.6200\n",
      "Straße: 0.6139\n",
      "Fabian: 0.6051\n",
      "Mami: 0.5899\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarities = cosine_similarity([text_embedding], mobilitaets_embeddings)\n",
    "\n",
    "# Find most similar terms\n",
    "similarity_scores = similarities[0]  # Since there's only one comparison text\n",
    "similarity_dict = {mobilitaetsbegriffe[i]: similarity_scores[i] for i in range(len(mobilitaetsbegriffe))}\n",
    "\n",
    "# Sort by highest similarity\n",
    "sorted_similarity = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Output mobility terms by similarity\n",
    "for term, score in sorted_similarity:\n",
    "    print(f\"{term}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
