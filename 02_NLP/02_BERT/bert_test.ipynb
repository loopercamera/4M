{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-0.2487,  0.1334]])\n",
      "Vorhersage: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n",
    "\n",
    "# Beispieltext\n",
    "text = \"Verkehrsdaten in Berlin\"\n",
    "\n",
    "# Text tokenisieren\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Vorhersage machen (ungewöhnlich gut aufgrund der nicht trainierten Klassifikationsschicht)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Ergebnisse (Logits)\n",
    "logits = outputs.logits\n",
    "print(f\"Logits: {logits}\")\n",
    "\n",
    "# Vorhersage (Index der höchsten Wahrscheinlichkeit)\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "print(f\"Vorhersage: {prediction.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-german-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für Mobilitätsbegriffe\n",
    "mobilitaetsbegriffe = [\n",
    "    \"Verkehr\", \"Transport\", \"Fahrzeug\", \"Straße\", \"Fahrplan\", \"Auto\", \"Bahn\", \"Mobilität\",\n",
    "    \"Fahrrad\", \"Bus\", \"E-Mobilität\", \"ÖPNV\", \"Flughafen\", \"Lkw\", \"Velo\", \"Fabian\", \"XYZ\", \"Essen\", \"Mami\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    # Tokenisieren und in Tensoren umwandeln\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Vorwärtsdurchlauf durch das Modell\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Wir nehmen den [CLS]-Token als Repräsentation des Textes\n",
    "    # outputs.last_hidden_state ist ein Tensor der Form (batch_size, sequence_length, hidden_size)\n",
    "    # Wir nehmen den ersten Token ([CLS]-Token) und extrahieren das Embedding\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Beispieltext\n",
    "text = \"Reiseverhalten\"\n",
    "\n",
    "# Embedding des Beispieltextes berechnen\n",
    "text_embedding = get_embedding(text)\n",
    "\n",
    "# Embedding der Mobilitätsbegriffe berechnen\n",
    "mobilitaets_embeddings = [get_embedding(begriff) for begriff in mobilitaetsbegriffe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilität: 0.8375\n",
      "E-Mobilität: 0.8211\n",
      "Verkehr: 0.7885\n",
      "Auto: 0.7408\n",
      "Fahrplan: 0.7294\n",
      "Velo: 0.7192\n",
      "Fahrrad: 0.7171\n",
      "Transport: 0.7162\n",
      "Lkw: 0.7160\n",
      "Essen: 0.7158\n",
      "Fahrzeug: 0.6914\n",
      "ÖPNV: 0.6898\n",
      "Bahn: 0.6739\n",
      "XYZ: 0.6719\n",
      "Bus: 0.6578\n",
      "Flughafen: 0.6200\n",
      "Straße: 0.6139\n",
      "Fabian: 0.6051\n",
      "Mami: 0.5899\n"
     ]
    }
   ],
   "source": [
    "# Kosinusähnlichkeit berechnen\n",
    "similarities = cosine_similarity([text_embedding], mobilitaets_embeddings)\n",
    "\n",
    "# Ähnlichsten Begriffe finden\n",
    "similarity_scores = similarities[0]  # Da es nur einen Vergleichstext gibt\n",
    "similarity_dict = {mobilitaetsbegriffe[i]: similarity_scores[i] for i in range(len(mobilitaetsbegriffe))}\n",
    "\n",
    "# Sortiere nach der höchsten Ähnlichkeit\n",
    "sorted_similarity = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Ausgabe der Mobilitätsbegriffe nach Ähnlichkeit\n",
    "for begriff, score in sorted_similarity:\n",
    "    print(f\"{begriff}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Difference found from training, BERT seems not to be what we search for. For Word embeddings and word2vec in german may exist better libraries/models\n",
    "\n",
    "## May try different approach using BERT, see other files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
