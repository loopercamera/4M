{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417e9639",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook labels all the data from the csv which may be used for upcoming testing purposes (e.g. 04_Clustering)\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded79812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import time\n",
    "\n",
    "inputdata_file = 'data/merged_dataset_metadata.csv'\n",
    "outputdata_file ='data/merged_dataset_metadata_labelled.csv'\n",
    "\n",
    "with open(\"data/apikeys.json\") as f:\n",
    "    config = json.load(f)\n",
    "API_KEYS = config[\"GOOGLE_API_KEYS\"]\n",
    "API_KEYS_CYCLE = cycle(API_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 1: Number of rows with only filled dataset_description_DE and dataset_title_DE: 20213\n",
      "Index 2: Remaining number of rows with only filled dataset_description_DE: 68\n",
      "Index 3: Remaining number of rows with only filled dataset_description_EN and dataset_title_EN: 500\n",
      "Index 4: Remaining number of rows with only filled dataset_description_EN: 0\n",
      "Index 5: Remaining number of rows with only filled dataset_description_FR and dataset_title_FR: 1304\n",
      "Index 6: Remaining number of rows with only filled dataset_description_FR: 0\n",
      "Index 7: Remaining number of rows with only filled dataset_description_IT and dataset_title_IT: 30\n",
      "Index 8: Remaining number of rows with only filled dataset_description_IT: 0\n",
      "Index 9: Remaining number of rows with only filled dataset_description and dataset_title: 4611\n",
      "Index 10: Remaining number of rows with only filled dataset_description: 0\n",
      "Index 11: Remaining number of rows with only filled dataset_title_DE: 1697\n",
      "Index 12: Remaining number of rows with only filled dataset_title_EN: 2\n",
      "Index 13: Remaining number of rows with only filled dataset_title_FR: 0\n",
      "Index 14: Remaining number of rows with only filled dataset_title_IT: 0\n",
      "Index 15: Remaining number of rows with only filled dataset_title: 444\n",
      "Index 16: Remaining number of rows with only filled dataset_description_UNKNOWN and dataset_title_UNKNOWN: 0\n",
      "Index 17: Remaining number of rows with only filled dataset_description_UNKNOWN: 0\n",
      "Index 18: Remaining number of rows with only filled dataset_title_UNKNOWN: 210\n",
      "All rows (29079/29079) have been successfully assigned an iteration_index.\n"
     ]
    }
   ],
   "source": [
    "# Assigning an index to each row based on the presence of content in specific cells\n",
    "\n",
    "# Index 1:\n",
    "df['iteration_index'] = np.where(\n",
    "    (~df['dataset_description_DE'].isna() & (df['dataset_description_DE'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_DE'].isna() & (df['dataset_title_DE'].astype(str).str.strip() != '')),\n",
    "    1,\n",
    "    None\n",
    ")\n",
    "print(f\"Index 1: Number of rows with only filled dataset_description_DE and dataset_title_DE: {(df['iteration_index'] == 1).sum()}\")\n",
    "\n",
    "# Index 2:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_DE'].isna() & (df['dataset_description_DE'].astype(str).str.strip() != '')),\n",
    "    2,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 2: Remaining number of rows with only filled dataset_description_DE: {(df['iteration_index'] == 2).sum()}\")\n",
    "\n",
    "# Index 3:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_EN'].isna() & (df['dataset_description_EN'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_EN'].isna() & (df['dataset_title_EN'].astype(str).str.strip() != '')),\n",
    "    3,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 3: Remaining number of rows with only filled dataset_description_EN and dataset_title_EN: {(df['iteration_index'] == 3).sum()}\")\n",
    "\n",
    "# Index 4:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_EN'].isna() & (df['dataset_description_EN'].astype(str).str.strip() != '')),\n",
    "    4,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 4: Remaining number of rows with only filled dataset_description_EN: {(df['iteration_index'] == 4).sum()}\")\n",
    "\n",
    "# Index 5:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_FR'].isna() & (df['dataset_description_FR'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_FR'].isna() & (df['dataset_title_FR'].astype(str).str.strip() != '')),\n",
    "    5,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 5: Remaining number of rows with only filled dataset_description_FR and dataset_title_FR: {(df['iteration_index'] == 5).sum()}\")\n",
    "\n",
    "# Index 6:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_FR'].isna() & (df['dataset_description_FR'].astype(str).str.strip() != '')),\n",
    "    6,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 6: Remaining number of rows with only filled dataset_description_FR: {(df['iteration_index'] == 6).sum()}\")\n",
    "\n",
    "# Index 7:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_IT'].isna() & (df['dataset_description_IT'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_IT'].isna() & (df['dataset_title_IT'].astype(str).str.strip() != '')),\n",
    "    7,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 7: Remaining number of rows with only filled dataset_description_IT and dataset_title_IT: {(df['iteration_index'] == 7).sum()}\")\n",
    "\n",
    "# Index 8:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_IT'].isna() & (df['dataset_description_IT'].astype(str).str.strip() != '')),\n",
    "    8,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 8: Remaining number of rows with only filled dataset_description_IT: {(df['iteration_index'] == 8).sum()}\")\n",
    "\n",
    "# Index 9:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_RM'].isna() & (df['dataset_description_RM'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_RM'].isna() & (df['dataset_title_RM'].astype(str).str.strip() != '')),\n",
    "    9,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 9: Remaining number of rows with only filled dataset_description_RM and dataset_title_RM: {(df['iteration_index'] == 9).sum()}\")\n",
    "\n",
    "# Index 10:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_RM'].isna() & (df['dataset_description_RM'].astype(str).str.strip() != '')),\n",
    "    10,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 10: Remaining number of rows with only filled dataset_description_RM: {(df['iteration_index'] == 10).sum()}\")\n",
    "\n",
    "# Index 11:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_DE'].isna() & (df['dataset_title_DE'].astype(str).str.strip() != '')),\n",
    "    11,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 11: Remaining number of rows with only filled dataset_title_DE: {(df['iteration_index'] == 11).sum()}\")\n",
    "\n",
    "# Index 12:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_EN'].isna() & (df['dataset_title_EN'].astype(str).str.strip() != '')),\n",
    "    12,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 12: Remaining number of rows with only filled dataset_title_EN: {(df['iteration_index'] == 12).sum()}\")\n",
    "\n",
    "# Index 13:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_FR'].isna() & (df['dataset_title_FR'].astype(str).str.strip() != '')),\n",
    "    13,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 13: Remaining number of rows with only filled dataset_title_FR: {(df['iteration_index'] == 13).sum()}\")\n",
    "\n",
    "# Index 14:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_IT'].isna() & (df['dataset_title_IT'].astype(str).str.strip() != '')),\n",
    "    14,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 14: Remaining number of rows with only filled dataset_title_IT: {(df['iteration_index'] == 14).sum()}\")\n",
    "\n",
    "# Index 15:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_RM'].isna() & (df['dataset_title_RM'].astype(str).str.strip() != '')),\n",
    "    15,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 15: Remaining number of rows with only filled dataset_title_RM: {(df['iteration_index'] == 15).sum()}\")\n",
    "\n",
    "# Index 16:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_UNKNOWN'].isna() & (df['dataset_description_UNKNOWN'].astype(str).str.strip() != '')) &\n",
    "    (~df['dataset_title_UNKNOWN'].isna() & (df['dataset_title_UNKNOWN'].astype(str).str.strip() != '')),\n",
    "    16,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 16: Remaining number of rows with only filled dataset_description_UNKNOWN and dataset_title_UNKNOWN: {(df['iteration_index'] == 16).sum()}\")\n",
    "\n",
    "# Index 17:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_description_UNKNOWN'].isna() & (df['dataset_description_UNKNOWN'].astype(str).str.strip() != '')),\n",
    "    17,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 17: Remaining number of rows with only filled dataset_description_UNKNOWN: {(df['iteration_index'] == 17).sum()}\")\n",
    "\n",
    "# Index 18:\n",
    "df['iteration_index'] = np.where(\n",
    "    df['iteration_index'].isna() &\n",
    "    (~df['dataset_title_UNKNOWN'].isna() & (df['dataset_title_UNKNOWN'].astype(str).str.strip() != '')),\n",
    "    18,\n",
    "    df['iteration_index']\n",
    ")\n",
    "print(f\"Index 18: Remaining number of rows with only filled dataset_title_UNKNOWN: {(df['iteration_index'] == 18).sum()}\")\n",
    "\n",
    "# Check if all rows have been assigned an index.\n",
    "if (df['iteration_index'] < 20).sum() < len(df):\n",
    "    missing_rows = df[df['iteration_index'].isna()]\n",
    "    print(\"ERROR: Not all rows have been assigned an iteration_index.\")\n",
    "    print(f\"Number of rows without index: {len(missing_rows)}\")\n",
    "    print(\"Example rows without index:\")\n",
    "    print(missing_rows.head(5))\n",
    "else:\n",
    "    print(f\"All rows ({(df['iteration_index'] < 20).sum()}/{len(df)}) have been successfully assigned an iteration_index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cada582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 1 with 20213 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4043/4043 [1:27:16<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 2 with 68 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 3 with 500 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 5 with 1304 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [05:37<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 7 with 30 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 9 with 4611 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 923/923 [19:49<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 11 with 1697 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [07:15<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 12 with 2 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 15 with 444 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [01:53<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 18 with 210 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:53<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying 580 ERROR rows with chunk_size = 1\n",
      "Retrying group 1 with 455 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [09:05<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 3 with 30 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 5 with 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 7 with 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 9 with 65 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:18<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 11 with 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "time.sleep(7200)\n",
    "\n",
    "key_count = len(API_KEYS)\n",
    "chunk_size = 5 # Number of rows sent to the LLM at once (best results with 1, but not suitable for free access/trial limits)\n",
    "requests_per_key = 15 # Number of requests per key in a minute \n",
    "\n",
    "current_key_index = 0\n",
    "key_request_counter = 0\n",
    "cycle_start_time = time.time()\n",
    "\n",
    "# Different chunk lines depending on the iteration index\n",
    "group_chunk_lines = {\n",
    "    \"1\": lambda row: f\"Titel: {row['dataset_title_DE']}\\nBeschreibung: {row['dataset_description_DE']}\",\n",
    "    \"2\": lambda row: f\"Beschreibung: {row['dataset_description_DE']}\",\n",
    "    \"3\": lambda row: f\"Titel: {row['dataset_title_EN']}\\nBeschreibung: {row['dataset_description_EN']}\",\n",
    "    \"4\": lambda row: f\"Beschreibung: {row['dataset_description_EN']}\",\n",
    "    \"5\": lambda row: f\"Titel: {row['dataset_title_FR']}\\nBeschreibung: {row['dataset_description_FR']}\",\n",
    "    \"6\": lambda row: f\"Beschreibung: {row['dataset_description_FR']}\",\n",
    "    \"7\": lambda row: f\"Titel: {row['dataset_title_IT']}\\nBeschreibung: {row['dataset_description_IT']}\",\n",
    "    \"8\": lambda row: f\"Beschreibung: {row['dataset_description_IT']}\",\n",
    "    \"9\": lambda row: f\"Titel: {row['dataset_title_RM']}\\nBeschreibung: {row['dataset_description_RM']}\",\n",
    "    \"10\": lambda row: f\"Beschreibung: {row['dataset_description_RM']}\",\n",
    "    \"11\": lambda row: f\"Titel: {row['dataset_title_DE']}\",\n",
    "    \"12\": lambda row: f\"Titel: {row['dataset_title_EN']}\",\n",
    "    \"13\": lambda row: f\"Titel: {row['dataset_title_FR']}\",\n",
    "    \"14\": lambda row: f\"Titel: {row['dataset_title_IT']}\",\n",
    "    \"15\": lambda row: f\"Titel: {row['dataset_title_RM']}\",\n",
    "    \"16\": lambda row: f\"Titel: {row['dataset_title_UNKNOWN']}\\nBeschreibung: {row['dataset_description_UNKNOWN']}\",\n",
    "    \"17\": lambda row: f\"Beschreibung: {row['dataset_description_UNKNOWN']}\",\n",
    "    \"18\": lambda row: f\"Titel: {row['dataset_title_UNKNOWN']}\",\n",
    "}\n",
    "\n",
    "relevant_columns = [\n",
    "    'dataset_title_DE', 'dataset_description_DE',\n",
    "    'dataset_title_EN', 'dataset_description_EN',\n",
    "    'dataset_title_FR', 'dataset_description_FR',\n",
    "    'dataset_title_IT', 'dataset_description_IT',\n",
    "    'dataset_title_RM', 'dataset_description_RM',\n",
    "    'dataset_title_UNKNOWN', 'dataset_description_UNKNOWN'\n",
    "]\n",
    "\n",
    "# Group by the 'index' column (or any other grouping criteria)\n",
    "for group_name, group_df in df.groupby('iteration_index'):\n",
    "    print(f\"Processing group: {group_name} with {len(group_df)} entries\")\n",
    "    \n",
    "    # Iterate in chunks within this group\n",
    "    for i in tqdm(range(0, len(group_df), chunk_size)):\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        chunk_df = group_df.iloc[i:i + chunk_size][relevant_columns]\n",
    "\n",
    "        formatter = group_chunk_lines[str(group_name)]\n",
    "        chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "        prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "        # Submit the prompt to the Gemini model\n",
    "        response = client.models.generate_content_stream(\n",
    "            model=\"gemini-2.0-flash-lite-001\",\n",
    "            contents=[prompt],\n",
    "            config=types.GenerateContentConfig(\n",
    "                max_output_tokens=chunk_size * 2,\n",
    "                temperature=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        result_text = \"\"\n",
    "        for chunk in response:\n",
    "            result_text += chunk.text\n",
    "\n",
    "        predictions = result_text.strip().splitlines()\n",
    "\n",
    "        if len(predictions) != len(chunk_df):\n",
    "            predictions = [] \n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "            continue\n",
    "\n",
    "        target_indices = chunk_df.index\n",
    "        df.loc[target_indices, 'mobilitydata_labelled'] = predictions\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)\n",
    "        \n",
    "\n",
    "# After the main loop: retry failed attempts with chunk_size = 1\n",
    "error_df = df[df['mobilitydata_labelled'] == 'ERROR']\n",
    "\n",
    "if not error_df.empty:\n",
    "    print(f\"Retrying {len(error_df)} ERROR rows with chunk_size = 1\")\n",
    "\n",
    "    for group_name, group_df in error_df.groupby('iteration_index'):\n",
    "        print(f\"Retrying group {group_name} with {len(group_df)} rows\")\n",
    "        for i in tqdm(range(0, len(group_df), 1)):  # chunk_size = 1\n",
    "            if key_request_counter >= requests_per_key:\n",
    "                current_key_index += 1\n",
    "                key_request_counter = 0\n",
    "\n",
    "                if current_key_index >= key_count:\n",
    "                    elapsed = time.time() - cycle_start_time\n",
    "                    if elapsed < 60:\n",
    "                        wait_time = int(60 - elapsed)\n",
    "                        print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    current_key_index = 0\n",
    "                    cycle_start_time = time.time()\n",
    "\n",
    "            CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "            client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "            chunk_df = group_df.iloc[i:i + 1][relevant_columns]\n",
    "            formatter = group_chunk_lines[str(group_name)]\n",
    "            chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "            prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "            response = client.models.generate_content_stream(\n",
    "                model=\"gemini-2.0-flash-lite-001\",\n",
    "                contents=[prompt],\n",
    "                config=types.GenerateContentConfig(\n",
    "                    max_output_tokens=1,\n",
    "                    temperature=0\n",
    "                )\n",
    "            )\n",
    "\n",
    "            result_text = \"\"\n",
    "            for chunk in response:\n",
    "                result_text += chunk.text\n",
    "\n",
    "            predictions = result_text.strip().splitlines()\n",
    "\n",
    "            if len(predictions) != 1:\n",
    "                df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "                continue\n",
    "\n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = predictions\n",
    "            key_request_counter += 1\n",
    "            time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b7fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been successfully saved as data/merged_dataset_metadata_labelled.csv.\n"
     ]
    }
   ],
   "source": [
    "# Write dataframe in new csv-File\n",
    "df.to_csv(outputdata_file, index=False)\n",
    "\n",
    "print(f'The file has been successfully saved as {outputdata_file}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
