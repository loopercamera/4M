{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1f3115",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook takes the labelled data from 01_Prototype_LM and labelles them by passing them through a prompt to the large language model (LLM) Google Gemini. Four different prompts are tested, each with batch sizes of 1, 5, and 10. The resulting twelve combinations are then compared to evaluate their performance.\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import time\n",
    "\n",
    "inputdata_file = '../01_Prototype_LM/data/03_labelled_data.csv' # the same file as in 01_Prototype_LM\n",
    "outputdata_file ='data/01_predicted_data.csv'\n",
    "\n",
    "with open(\"data/apikeys.json\") as f:\n",
    "    config = json.load(f)\n",
    "API_KEYS = config[\"GOOGLE_API_KEYS\"]\n",
    "API_KEYS_CYCLE = cycle(API_KEYS)\n",
    "\n",
    "# structure of apikey.json:\n",
    "# {\n",
    "#   \"GOOGLE_API_KEYS\": [\n",
    "#     \"key_1\",\n",
    "#     \"key_2\",\n",
    "#     ...\n",
    "#     \"key_n\"\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a551bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labelled rows after filtering: 150\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(inputdata_file, dtype={'mobilitydata_labelled': 'string'}, low_memory=False)\n",
    "\n",
    "# Drop rows where 'mobilitydata_labelled' is empty (NaN)\n",
    "df = df.dropna(subset=['mobilitydata_labelled'])\n",
    "\n",
    "# Convert 'mobilitydata_labelled' to boolean type\n",
    "df['mobilitydata_labelled'] = df['mobilitydata_labelled'].map({'True': True, 'False': False})\n",
    "\n",
    "# Print the number of rows remaining after filtering\n",
    "print(f\"Number of labelled rows after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846794c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for iterating through API keys\n",
    "requests_per_key = 15  # Max requests allowed per API key\n",
    "key_count = len(API_KEYS)\n",
    "current_key_index = 0  # Index of the currently used API key\n",
    "key_request_counter = 0  # Counter for requests per key\n",
    "cycle_start_time = time.time()  # Timestamp for rate-limiting\n",
    "\n",
    "# All prompts\n",
    "def build_prompt(prompt_id, chunk_lines):\n",
    "    if prompt_id == 1:\n",
    "        # Basic prompt with T/F format\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\"\n",
    "            \"Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) +\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "    elif prompt_id == 2:\n",
    "        # Prompt with T/F/U format (uncertainty included)\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\"\n",
    "            \"Antworte nur mit T (True), F (False) oder U (Uncertain).\\n\\n\" + \"\\n\\n\".join(chunk_lines) +\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "    elif prompt_id == 3:\n",
    "        # Compact format, answers concatenated without space or punctuation\n",
    "        return (\n",
    "            \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?\\n\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines) + \"\\n\\n\" +\n",
    "            \"Antworte nur mit T für True, F für False oder U für Uncertain und reihe alle Antworten direkt aneinander ohne Leerzeichen, Umbrüche, Texte oder Sonderzeichen zu verwenden. Beispielhaftes Antwortschema:\\n\\n\" +\n",
    "            \"TFTFFTFUTT\"\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "\n",
    "    elif prompt_id == 4:\n",
    "        # Expert-level instruction with clear format and rules\n",
    "        return (\n",
    "            \"Als Experte für Datenannotation im Bereich Mobilitäts- und Verkehrsdaten ist es Ihre Aufgabe, öffentliche Datensätze daraufhin zu prüfen, ob sie Informationen enthalten, die eindeutig Mobilitäts- oder Verkehrsdaten betreffen. Ihre Einschätzung hilft bei der sachgerechten Klassifizierung dieser Inhalte auf einem nationalen Datenportal.\\n\"\n",
    "            \"Aufgabe:\\n\" \n",
    "            \"Beurteilen Sie, ob es sich bei dem folgenden Datensatz um Mobilitäts- oder Verkehrsdaten handelt.\\n\\n\"\n",
    "            \"Antwortformat:\\n\"  \n",
    "            \"Antworten Sie **nur mit einer der folgenden Optionen**, ohne zusätzliche Zeichen oder Erläuterungen (T für True, F für False, U für Uncertain):\\n\\n\"\n",
    "            \"- T\\n\"  \n",
    "            \"- F\\n\"  \n",
    "            \"- U\\n\\n\"\n",
    "            \"Hinweise:\\n\" \n",
    "            \"- Sollten mehrer Anfragen auf einmal verarbeitet werden, so antworten sie im folgenden Format (Beispiel für 10 Anfragen): TFTFFTFUTT\"\n",
    "            \"- Verwenden Sie **U: Uncertain**, wenn die Informationen im Titel oder in der Beschreibung unklar oder nicht ausreichend sind.\\n\"  \n",
    "            \"- Berücksichtigen Sie Aspekte wie Verkehrsmittel, Infrastruktur, Mobilitätsverhalten oder Verkehrsfluss.\\n\"  \n",
    "            \"- Geben Sie keine zusätzlichen Erläuterungen – nur die ausgewählte Option.\\n\\n\"\n",
    "            \"Datensatzbeschreibungen:\\n\" +\n",
    "            \"\\n\\n\".join(chunk_lines) +\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Ungültige Prompt-ID\")\n",
    "\n",
    "# Process a chunk of rows with the selected prompt\n",
    "def process_chunks(df, indices, chunk_size, prompt_id):\n",
    "    global current_key_index, key_request_counter, cycle_start_time\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(indices), chunk_size)):\n",
    "        # Rotate API key if limit is reached\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            # If all keys exhausted, wait before restarting\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Max requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        batch_indices = indices[i:i + chunk_size]\n",
    "        chunk_df = df.loc[batch_indices][['dataset_title_DE', 'dataset_description_DE']]\n",
    "\n",
    "        # Combine title and description into formatted text\n",
    "        chunk_lines = chunk_df.apply(\n",
    "            lambda row: f\"Titel: {row['dataset_title_DE']}\\nBeschreibung: {row['dataset_description_DE']}\",\n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        prompt = build_prompt(prompt_id, chunk_lines)\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.models.generate_content_stream(\n",
    "                    model=\"gemini-2.0-flash-lite-001\",\n",
    "                    contents=[prompt],\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        max_output_tokens=chunk_size,\n",
    "                        temperature=0,\n",
    "                        top_p=0.95,  # Default value\n",
    "                        top_k=64,    # Default value\n",
    "                        candidate_count=1  # Default value\n",
    "                    )\n",
    "                )\n",
    "                result_text = \"\".join(chunk.text for chunk in response)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"Rate limit reached. Waiting 60 seconds... (Attempt {attempt+1} of {max_retries})\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: {error_str}\")\n",
    "                    break\n",
    "        else:\n",
    "            # All retries failed – store error result for each row\n",
    "            for idx in batch_indices:\n",
    "                results.append({\n",
    "                    \"index\": idx,\n",
    "                    \"mobilitydata_generated\": \"ERROR\",\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"chunk_size\": chunk_size\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        # Split model result into individual predictions\n",
    "        predictions = list(result_text.strip())\n",
    "        for rel_idx, prediction in zip(batch_indices, predictions):\n",
    "            results.append({\n",
    "                \"index\": rel_idx,\n",
    "                \"mobilitydata_generated\": prediction if prediction in [\"T\", \"F\", \"U\"] else \"ERROR\",\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"chunk_size\": chunk_size\n",
    "            })\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)  # Small delay to avoid hitting rate limits\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize result column\n",
    "df['mobilitydata_generated'] = None\n",
    "all_indices = df.index.tolist()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Create result columns for each prompt and chunk combination\n",
    "for prompt_id in range(1, 5):\n",
    "    for chunk_size in [10, 5, 1]:\n",
    "        col_name = f\"mobilitydata_generated_p{prompt_id}_c{chunk_size}\"\n",
    "        df[col_name] = None\n",
    "\n",
    "# Run model for all combinations and collect results\n",
    "for prompt_id in range(1, 5):\n",
    "    for chunk_size in [10, 5, 1]:\n",
    "        print(f\"Running Prompt {prompt_id} with chunk_size {chunk_size}\")\n",
    "        result_rows = process_chunks(df.copy(), all_indices, chunk_size, prompt_id)\n",
    "        all_results.extend(result_rows)\n",
    "\n",
    "# Write results into the corresponding columns\n",
    "for row in all_results:\n",
    "    idx = row['index']\n",
    "    prompt_id = row['prompt_id']\n",
    "    chunk_size = row['chunk_size']\n",
    "    value = row['mobilitydata_generated']\n",
    "    col_name = f\"mobilitydata_generated_p{prompt_id}_c{chunk_size}\"\n",
    "    df.at[idx, col_name] = value\n",
    "\n",
    "# Replace empty or invalid values with \"ERROR\"\n",
    "for prompt_id in range(1, 5):\n",
    "    for chunk_size in [10, 5, 1]:\n",
    "        col_name = f\"mobilitydata_generated_p{prompt_id}_c{chunk_size}\"\n",
    "        df[col_name] = df[col_name].apply(lambda x: x if x in [\"T\", \"F\", \"U\", \"ERROR\"] else \"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a804b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = []\n",
    "\n",
    "# Automatically identify result columns\n",
    "result_columns = [col for col in df.columns if col.startswith(\"mobilitydata_generated_p\")]\n",
    "\n",
    "for col in sorted(result_columns):\n",
    "    try:\n",
    "        parts = col.split(\"_\")\n",
    "        prompt_id = int(parts[2][1:])   # e.g. 'p1' → 1\n",
    "        chunk_size = int(parts[3][1:])  # e.g. 'c10' → 10\n",
    "\n",
    "        # Filter valid predictions (only T/F)\n",
    "        valid = df[df[col].isin(['T', 'F'])].copy()\n",
    "        valid['prediction'] = valid[col].map({'T': True, 'F': False})\n",
    "\n",
    "        # Additional counts for uncertain and error values\n",
    "        count_u = df[col].eq('U').sum()\n",
    "        count_error = df[col].eq('ERROR').sum()\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        tp = ((valid['mobilitydata_labelled'] == True) & (valid['prediction'] == True)).sum()\n",
    "        tn = ((valid['mobilitydata_labelled'] == False) & (valid['prediction'] == False)).sum()\n",
    "        fp = ((valid['mobilitydata_labelled'] == False) & (valid['prediction'] == True)).sum()\n",
    "        fn = ((valid['mobilitydata_labelled'] == True) & (valid['prediction'] == False)).sum()\n",
    "        total = len(valid)\n",
    "\n",
    "        accuracy = (tp + tn) / total if total > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        analysis.append({\n",
    "            'prompt_id': prompt_id,\n",
    "            'chunk_size': chunk_size,\n",
    "            'TP': tp,\n",
    "            'TN': tn,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Total': total,\n",
    "            'U': count_u,\n",
    "            'ERROR': count_error,\n",
    "            'Accuracy': round(accuracy, 4),\n",
    "            'Precision': round(precision, 4),\n",
    "            'Recall': round(recall, 4),\n",
    "            'F1-Score': round(f1_score, 4)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # Skip column in case of processing error\n",
    "        print(f\"Überspringe Spalte {col} wegen Fehler: {e}\")\n",
    "\n",
    "# Create and sort the evaluation results as DataFrame\n",
    "analysis_df = pd.DataFrame(analysis)\n",
    "analysis_df = analysis_df.sort_values(['prompt_id', 'chunk_size'])\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nAuswertung der Prompt-/Chunk-Kombinationen:\")\n",
    "print(analysis_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2793e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe in new csv-File\n",
    "df.to_csv(outputdata_file, index=False)\n",
    "\n",
    "print(f'The file has been successfully saved as {outputdata_file}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
