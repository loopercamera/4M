{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868c5fd1",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook performs labeling on all data extracted from the database.\n",
    "\n",
    "See 02_Prototype_LLM for implementation details.\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ecb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import time\n",
    "\n",
    "with open(\"data/apikeys.json\") as f:\n",
    "    config = json.load(f)\n",
    "API_KEYS = config[\"GOOGLE_API_KEYS\"]\n",
    "API_KEYS_CYCLE = cycle(API_KEYS)\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def generate_columns(identifier, base_cols, prefixes, extra_columns=None):\n",
    "    language_columns = [f\"{col}_{lang.lower()}\" for lang in prefixes for col in base_cols]\n",
    "    columns = [identifier] + language_columns\n",
    "    if extra_columns:\n",
    "        columns += extra_columns\n",
    "    return columns\n",
    "\n",
    "def fetch_data(config, db_name, table, columns, limit=None):\n",
    "    config[\"dbname\"] = db_name\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        sql = f\"SELECT {', '.join(columns)} FROM {table}\"\n",
    "        if limit:\n",
    "            sql += f\" LIMIT {limit}\"\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Laden der Daten:\", e)\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "def update_data(df, identifier_column, value_column, new_value):\n",
    "    df.loc[:, value_column] = new_value\n",
    "    return df\n",
    "\n",
    "def write_back_to_db(config, db_name, table, df, identifier_column, batch_size=1000):\n",
    "    config[\"dbname\"] = db_name\n",
    "    total_rows = len(df)\n",
    "    try:\n",
    "        for start in range(0, total_rows, batch_size):\n",
    "            end = min(start + batch_size, total_rows)\n",
    "            batch_df = df.iloc[start:end]\n",
    "\n",
    "            conn = psycopg2.connect(**config)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            for _, row in batch_df.iterrows():\n",
    "                cur.execute(\n",
    "                    f\"UPDATE {table} SET {', '.join([f'{col} = %s' for col in df.columns if col != identifier_column])} \"\n",
    "                    f\"WHERE {identifier_column} = %s\",\n",
    "                    [row[col] for col in df.columns if col != identifier_column] + [row[identifier_column]]\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "\n",
    "            print(f\"Batch {start}–{end} erfolgreich aktualisiert.\")\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Zurückschreiben der Daten:\", e)\n",
    "\n",
    "def assign_iteration_index(df):\n",
    "    df = df.copy()\n",
    "    df['iteration_index'] = None\n",
    "    \n",
    "    def not_empty(col):\n",
    "        return ~df[col].isna() & (df[col].astype(str).str.strip() != '')\n",
    "\n",
    "    index_rules = [\n",
    "        (1,  not_empty('dataset_description_de') & not_empty('dataset_title_de')),\n",
    "        (2,  not_empty('dataset_description_de')),\n",
    "        (3,  not_empty('dataset_description_en') & not_empty('dataset_title_en')),\n",
    "        (4,  not_empty('dataset_description_en')),\n",
    "        (5,  not_empty('dataset_description_fr') & not_empty('dataset_title_fr')),\n",
    "        (6,  not_empty('dataset_description_fr')),\n",
    "        (7,  not_empty('dataset_description_it') & not_empty('dataset_title_it')),\n",
    "        (8,  not_empty('dataset_description_it')),\n",
    "        (9,  not_empty('dataset_description_rm') & not_empty('dataset_title_rm')),\n",
    "        (10, not_empty('dataset_description_rm')),\n",
    "        (11, not_empty('dataset_title_de')),\n",
    "        (12, not_empty('dataset_title_en')),\n",
    "        (13, not_empty('dataset_title_fr')),\n",
    "        (14, not_empty('dataset_title_it')),\n",
    "        (15, not_empty('dataset_title_rm')),\n",
    "        (16, not_empty('dataset_description_unknown') & not_empty('dataset_title_unknown')),\n",
    "        (17, not_empty('dataset_description_unknown')),\n",
    "        (18, not_empty('dataset_title_unknown')),\n",
    "    ]\n",
    "\n",
    "    for index, condition in index_rules:\n",
    "        df.loc[df['iteration_index'].isna() & condition, 'iteration_index'] = index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31cf936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haabs\\AppData\\Local\\Temp\\ipykernel_58744\\3431336175.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Beispielnutzung\n",
    "config_path = r\"data/db_config.json\"\n",
    "db_name = \"4M\"\n",
    "table = \"merged_dataset_metadata\"\n",
    "identifier = \"dataset_identifier\"\n",
    "base_cols = [\"dataset_title\", \"dataset_keyword\", \"dataset_description\"]\n",
    "prefixes = [\"DE\", \"EN\", \"FR\", \"IT\", \"RM\", \"UNKNOWN\"]\n",
    "ismobility = [\"dataset_is_mobility\"]\n",
    "\n",
    "config = load_config(config_path)\n",
    "columns = generate_columns(identifier, base_cols, prefixes, extra_columns=ismobility)\n",
    "df = fetch_data(config, db_name, table, columns) #limit = 10000\n",
    "\n",
    "# Only process rows that do not already have a True/False value assigned\n",
    "df = df[df['dataset_is_mobility'].isna()]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6f7aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns an 'iteration_index' to each row based on available title/description content in various languages.\n",
    "df = assign_iteration_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9dabf90",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mobilitydata_labelled'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\haabs\\anaconda3\\envs\\bth\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mobilitydata_labelled'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 110\u001b[0m\n\u001b[0;32m    106\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# After the main loop: retry failed attempts with chunk_size = 1\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m error_df \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmobilitydata_labelled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(error_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ERROR rows with chunk_size = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haabs\\anaconda3\\envs\\bth\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\haabs\\anaconda3\\envs\\bth\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mobilitydata_labelled'"
     ]
    }
   ],
   "source": [
    "key_count = len(API_KEYS)\n",
    "chunk_size = 5 # Number of rows sent to the LLM at once (best results with 1, but not suitable for free access/trial limits)\n",
    "requests_per_key = 15 # Number of requests per key in a minute \n",
    "\n",
    "current_key_index = 0\n",
    "key_request_counter = 0\n",
    "cycle_start_time = time.time()\n",
    "\n",
    "# Different chunk lines depending on the iteration index\n",
    "group_chunk_lines = {\n",
    "    \"1\": lambda row: f\"Titel: {row['dataset_title_de']}\\nBeschreibung: {row['dataset_description_de']}\",\n",
    "    \"2\": lambda row: f\"Beschreibung: {row['dataset_description_de']}\",\n",
    "    \"3\": lambda row: f\"Titel: {row['dataset_title_en']}\\nBeschreibung: {row['dataset_description_en']}\",\n",
    "    \"4\": lambda row: f\"Beschreibung: {row['dataset_description_en']}\",\n",
    "    \"5\": lambda row: f\"Titel: {row['dataset_title_fr']}\\nBeschreibung: {row['dataset_description_fr']}\",\n",
    "    \"6\": lambda row: f\"Beschreibung: {row['dataset_description_fr']}\",\n",
    "    \"7\": lambda row: f\"Titel: {row['dataset_title_it']}\\nBeschreibung: {row['dataset_description_it']}\",\n",
    "    \"8\": lambda row: f\"Beschreibung: {row['dataset_description_it']}\",\n",
    "    \"9\": lambda row: f\"Titel: {row['dataset_title_rm']}\\nBeschreibung: {row['dataset_description_rm']}\",\n",
    "    \"10\": lambda row: f\"Beschreibung: {row['dataset_description_rm']}\",\n",
    "    \"11\": lambda row: f\"Titel: {row['dataset_title_de']}\",\n",
    "    \"12\": lambda row: f\"Titel: {row['dataset_title_en']}\",\n",
    "    \"13\": lambda row: f\"Titel: {row['dataset_title_fr']}\",\n",
    "    \"14\": lambda row: f\"Titel: {row['dataset_title_it']}\",\n",
    "    \"15\": lambda row: f\"Titel: {row['dataset_title_rm']}\",\n",
    "    \"16\": lambda row: f\"Titel: {row['dataset_title_unknown']}\\nBeschreibung: {row['dataset_description_unknown']}\",\n",
    "    \"17\": lambda row: f\"Beschreibung: {row['dataset_description_unknown']}\",\n",
    "    \"18\": lambda row: f\"Titel: {row['dataset_title_unknown']}\",\n",
    "}\n",
    "\n",
    "relevant_columns = [\n",
    "    'dataset_title_de', 'dataset_description_de',\n",
    "    'dataset_title_en', 'dataset_description_en',\n",
    "    'dataset_title_fr', 'dataset_description_fr',\n",
    "    'dataset_title_it', 'dataset_description_it',\n",
    "    'dataset_title_rm', 'dataset_description_rm',\n",
    "    'dataset_title_unknown', 'dataset_description_unknown'\n",
    "]\n",
    "\n",
    "# Group by the 'index' column (or any other grouping criteria)\n",
    "for group_name, group_df in df.groupby('iteration_index'):\n",
    "    print(f\"Processing group: {group_name} with {len(group_df)} entries\")\n",
    "    \n",
    "    # Iterate in chunks within this group\n",
    "    for i in tqdm(range(0, len(group_df), chunk_size)):\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        chunk_df = group_df.iloc[i:i + chunk_size][relevant_columns]\n",
    "\n",
    "        formatter = group_chunk_lines[str(group_name)]\n",
    "        chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "        prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "        # Submit the prompt to the Gemini model\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.models.generate_content_stream(\n",
    "                    model=\"gemini-2.0-flash-lite-001\",\n",
    "                    contents=[prompt],\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        max_output_tokens=chunk_size * 2,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                )\n",
    "                result_text = \"\".join(chunk.text for chunk in response)\n",
    "                break \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"Rate limit reached. Waiting 60 seconds... (Attempt {attempt+1} of {max_retries})\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: {error_str}\")\n",
    "                    break\n",
    "        else:\n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "            continue\n",
    "\n",
    "        predictions = result_text.strip().splitlines()\n",
    "\n",
    "        if len(predictions) != len(chunk_df):\n",
    "            predictions = [] \n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "            continue\n",
    "\n",
    "        target_indices = chunk_df.index\n",
    "        df.loc[target_indices, 'mobilitydata_labelled'] = predictions\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)\n",
    "        \n",
    "\n",
    "# After the main loop: retry failed attempts with chunk_size = 1\n",
    "error_df = df[df['mobilitydata_labelled'] == 'ERROR']\n",
    "\n",
    "if not error_df.empty:\n",
    "    print(f\"Retrying {len(error_df)} ERROR rows with chunk_size = 1\")\n",
    "\n",
    "    for group_name, group_df in error_df.groupby('iteration_index'):\n",
    "        print(f\"Retrying group {group_name} with {len(group_df)} rows\")\n",
    "        for i in tqdm(range(0, len(group_df), 1)):  # chunk_size = 1\n",
    "            if key_request_counter >= requests_per_key:\n",
    "                current_key_index += 1\n",
    "                key_request_counter = 0\n",
    "\n",
    "                if current_key_index >= key_count:\n",
    "                    elapsed = time.time() - cycle_start_time\n",
    "                    if elapsed < 60:\n",
    "                        wait_time = int(60 - elapsed)\n",
    "                        print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    current_key_index = 0\n",
    "                    cycle_start_time = time.time()\n",
    "\n",
    "            CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "            client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "            chunk_df = group_df.iloc[i:i + 1][relevant_columns]\n",
    "            formatter = group_chunk_lines[str(group_name)]\n",
    "            chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "            prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = client.models.generate_content_stream(\n",
    "                        model=\"gemini-2.0-flash-lite-001\",\n",
    "                        contents=[prompt],\n",
    "                        config=types.GenerateContentConfig(\n",
    "                            max_output_tokens=chunk_size * 2,\n",
    "                            temperature=0\n",
    "                        )\n",
    "                    )\n",
    "                    result_text = \"\".join(chunk.text for chunk in response)\n",
    "                    break \n",
    "                except Exception as e:\n",
    "                    error_str = str(e)\n",
    "                    if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                        print(f\"Rate limit reached. Waiting 60 seconds... (Attempt {attempt+1} of {max_retries})\")\n",
    "                        time.sleep(60)\n",
    "                    else:\n",
    "                        print(f\"Error: {error_str}\")\n",
    "                        break\n",
    "            else:\n",
    "                df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "                continue\n",
    "\n",
    "            predictions = result_text.strip().splitlines()\n",
    "\n",
    "            if len(predictions) != 1:\n",
    "                df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "                continue\n",
    "\n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = predictions\n",
    "            key_request_counter += 1\n",
    "            time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0–156 erfolgreich aktualisiert.\n"
     ]
    }
   ],
   "source": [
    "# Convert the LLM output ('T' or 'F') into actual boolean values (True/False)\n",
    "df['mobilitydata_labelled'] = df['mobilitydata_labelled'].map({'T': True, 'F': False})\n",
    "\n",
    "# Keep only rows with valid boolean labels (exclude 'ERROR', NaN, or anything else)\n",
    "df = df[df['mobilitydata_labelled'].isin([True, False])].copy()\n",
    "\n",
    "# Update the 'dataset_is_mobility' column in the DataFrame with the new boolean labels\n",
    "df = update_data(df, identifier_column=identifier, value_column='dataset_is_mobility', new_value=df['mobilitydata_labelled'])\n",
    "\n",
    "# Drop temporary helper columns used for internal processing\n",
    "df.drop(columns=['iteration_index', 'mobilitydata_labelled'], inplace=True)\n",
    "\n",
    "# Write the updated data back to the database using batch updates\n",
    "write_back_to_db(config, db_name, table, df, identifier_column=identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
