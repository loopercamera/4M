{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868c5fd1",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook performs labeling on all data extracted from the database.\n",
    "\n",
    "See 02_Prototype_LLM for implementation details.\n",
    "\n",
    "The code was created with the assistance of ChatGPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import time\n",
    "\n",
    "with open(\"data/apikeys.json\") as f:\n",
    "    config = json.load(f)\n",
    "API_KEYS = config[\"GOOGLE_API_KEYS\"]\n",
    "API_KEYS_CYCLE = cycle(API_KEYS)\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def generate_columns(identifier, base_cols, prefixes, extra_columns=None):\n",
    "    language_columns = [f\"{col}_{lang.lower()}\" for lang in prefixes for col in base_cols]\n",
    "    columns = [identifier] + language_columns\n",
    "    if extra_columns:\n",
    "        columns += extra_columns\n",
    "    return columns\n",
    "\n",
    "def fetch_data(config, db_name, table, columns, limit=None):\n",
    "    config[\"dbname\"] = db_name\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        sql = f\"SELECT {', '.join(columns)} FROM {table}\"\n",
    "        if limit:\n",
    "            sql += f\" LIMIT {limit}\"\n",
    "        df = pd.read_sql_query(sql, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Laden der Daten:\", e)\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "def update_data(df, identifier_column, value_column, new_value):\n",
    "    df.loc[:, value_column] = new_value\n",
    "    return df\n",
    "\n",
    "def write_back_to_db(config, db_name, table, df, identifier_column, batch_size=1000):\n",
    "    config[\"dbname\"] = db_name\n",
    "    total_rows = len(df)\n",
    "    try:\n",
    "        for start in range(0, total_rows, batch_size):\n",
    "            end = min(start + batch_size, total_rows)\n",
    "            batch_df = df.iloc[start:end]\n",
    "\n",
    "            conn = psycopg2.connect(**config)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            for _, row in batch_df.iterrows():\n",
    "                cur.execute(\n",
    "                    f\"UPDATE {table} SET {', '.join([f'{col} = %s' for col in df.columns if col != identifier_column])} \"\n",
    "                    f\"WHERE {identifier_column} = %s\",\n",
    "                    [row[col] for col in df.columns if col != identifier_column] + [row[identifier_column]]\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "\n",
    "            print(f\"Batch {start}–{end} erfolgreich aktualisiert.\")\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Zurückschreiben der Daten:\", e)\n",
    "\n",
    "def assign_iteration_index(df):\n",
    "    df = df.copy()\n",
    "    df['iteration_index'] = None\n",
    "    \n",
    "    def not_empty(col):\n",
    "        return ~df[col].isna() & (df[col].astype(str).str.strip() != '')\n",
    "\n",
    "    index_rules = [\n",
    "        (1,  not_empty('dataset_description_de') & not_empty('dataset_title_de')),\n",
    "        (2,  not_empty('dataset_description_de')),\n",
    "        (3,  not_empty('dataset_description_en') & not_empty('dataset_title_en')),\n",
    "        (4,  not_empty('dataset_description_en')),\n",
    "        (5,  not_empty('dataset_description_fr') & not_empty('dataset_title_fr')),\n",
    "        (6,  not_empty('dataset_description_fr')),\n",
    "        (7,  not_empty('dataset_description_it') & not_empty('dataset_title_it')),\n",
    "        (8,  not_empty('dataset_description_it')),\n",
    "        (9,  not_empty('dataset_description_rm') & not_empty('dataset_title_rm')),\n",
    "        (10, not_empty('dataset_description_rm')),\n",
    "        (11, not_empty('dataset_title_de')),\n",
    "        (12, not_empty('dataset_title_en')),\n",
    "        (13, not_empty('dataset_title_fr')),\n",
    "        (14, not_empty('dataset_title_it')),\n",
    "        (15, not_empty('dataset_title_rm')),\n",
    "        (16, not_empty('dataset_description_unknown') & not_empty('dataset_title_unknown')),\n",
    "        (17, not_empty('dataset_description_unknown')),\n",
    "        (18, not_empty('dataset_title_unknown')),\n",
    "    ]\n",
    "\n",
    "    for index, condition in index_rules:\n",
    "        df.loc[df['iteration_index'].isna() & condition, 'iteration_index'] = index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haabs\\AppData\\Local\\Temp\\ipykernel_32392\\2544229405.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24728\n"
     ]
    }
   ],
   "source": [
    "# Beispielnutzung\n",
    "config_path = r\"data/db_config.json\"\n",
    "db_name = \"4M\"\n",
    "table = \"merged_dataset_metadata\"\n",
    "identifier = \"dataset_identifier\"\n",
    "base_cols = [\"dataset_title\", \"dataset_keyword\", \"dataset_description\"]\n",
    "prefixes = [\"DE\", \"EN\", \"FR\", \"IT\", \"RM\", \"UNKNOWN\"]\n",
    "ismobility = [\"dataset_is_mobility\"]\n",
    "\n",
    "config = load_config(config_path)\n",
    "columns = generate_columns(identifier, base_cols, prefixes, extra_columns=ismobility)\n",
    "df = fetch_data(config, db_name, table, columns)\n",
    "\n",
    "# Only process rows that do not already have a True/False value assigned\n",
    "df = df[df['dataset_is_mobility'].isna()]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns an 'iteration_index' to each row based on available title/description content in various languages.\n",
    "df = assign_iteration_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9dabf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 1 with 6 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 3 with 1 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 5 with 1 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 11 with 2 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying 7 ERROR rows with chunk_size = 1\n",
      "Retrying group 1 with 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying group 11 with 2 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "key_count = len(API_KEYS)\n",
    "chunk_size = 5 # Number of rows sent to the LLM at once (best results with 1, but not suitable for free access/trial limits)\n",
    "requests_per_key = 15 # Number of requests per key in a minute \n",
    "\n",
    "current_key_index = 0\n",
    "key_request_counter = 0\n",
    "cycle_start_time = time.time()\n",
    "\n",
    "# Different chunk lines depending on the iteration index\n",
    "group_chunk_lines = {\n",
    "    \"1\": lambda row: f\"Titel: {row['dataset_title_de']}\\nBeschreibung: {row['dataset_description_de']}\",\n",
    "    \"2\": lambda row: f\"Beschreibung: {row['dataset_description_de']}\",\n",
    "    \"3\": lambda row: f\"Titel: {row['dataset_title_en']}\\nBeschreibung: {row['dataset_description_en']}\",\n",
    "    \"4\": lambda row: f\"Beschreibung: {row['dataset_description_en']}\",\n",
    "    \"5\": lambda row: f\"Titel: {row['dataset_title_fr']}\\nBeschreibung: {row['dataset_description_fr']}\",\n",
    "    \"6\": lambda row: f\"Beschreibung: {row['dataset_description_fr']}\",\n",
    "    \"7\": lambda row: f\"Titel: {row['dataset_title_it']}\\nBeschreibung: {row['dataset_description_it']}\",\n",
    "    \"8\": lambda row: f\"Beschreibung: {row['dataset_description_it']}\",\n",
    "    \"9\": lambda row: f\"Titel: {row['dataset_title_rm']}\\nBeschreibung: {row['dataset_description_rm']}\",\n",
    "    \"10\": lambda row: f\"Beschreibung: {row['dataset_description_rm']}\",\n",
    "    \"11\": lambda row: f\"Titel: {row['dataset_title_de']}\",\n",
    "    \"12\": lambda row: f\"Titel: {row['dataset_title_en']}\",\n",
    "    \"13\": lambda row: f\"Titel: {row['dataset_title_fr']}\",\n",
    "    \"14\": lambda row: f\"Titel: {row['dataset_title_it']}\",\n",
    "    \"15\": lambda row: f\"Titel: {row['dataset_title_rm']}\",\n",
    "    \"16\": lambda row: f\"Titel: {row['dataset_title_unknown']}\\nBeschreibung: {row['dataset_description_unknown']}\",\n",
    "    \"17\": lambda row: f\"Beschreibung: {row['dataset_description_unknown']}\",\n",
    "    \"18\": lambda row: f\"Titel: {row['dataset_title_unknown']}\",\n",
    "}\n",
    "\n",
    "relevant_columns = [\n",
    "    'dataset_title_de', 'dataset_description_de',\n",
    "    'dataset_title_en', 'dataset_description_en',\n",
    "    'dataset_title_fr', 'dataset_description_fr',\n",
    "    'dataset_title_it', 'dataset_description_it',\n",
    "    'dataset_title_rm', 'dataset_description_rm',\n",
    "    'dataset_title_unknown', 'dataset_description_unknown'\n",
    "]\n",
    "\n",
    "# Group by the 'index' column (or any other grouping criteria)\n",
    "for group_name, group_df in df.groupby('iteration_index'):\n",
    "    print(f\"Processing group: {group_name} with {len(group_df)} entries\")\n",
    "    \n",
    "    # Iterate in chunks within this group\n",
    "    for i in tqdm(range(0, len(group_df), chunk_size)):\n",
    "        if key_request_counter >= requests_per_key:\n",
    "            current_key_index += 1\n",
    "            key_request_counter = 0\n",
    "\n",
    "            if current_key_index >= key_count:\n",
    "                elapsed = time.time() - cycle_start_time\n",
    "                if elapsed < 60:\n",
    "                    wait_time = int(60 - elapsed)\n",
    "                    print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time + 1)\n",
    "                current_key_index = 0\n",
    "                cycle_start_time = time.time()\n",
    "\n",
    "        CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "        client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "        chunk_df = group_df.iloc[i:i + chunk_size][relevant_columns]\n",
    "\n",
    "        formatter = group_chunk_lines[str(group_name)]\n",
    "        chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "        prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "        # Submit the prompt to the Gemini model\n",
    "        response = client.models.generate_content_stream(\n",
    "            model=\"gemini-2.0-flash-lite-001\",\n",
    "            contents=[prompt],\n",
    "            config=types.GenerateContentConfig(\n",
    "                max_output_tokens=chunk_size * 2,\n",
    "                temperature=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        result_text = \"\"\n",
    "        for chunk in response:\n",
    "            result_text += chunk.text\n",
    "\n",
    "        predictions = result_text.strip().splitlines()\n",
    "\n",
    "        if len(predictions) != len(chunk_df):\n",
    "            predictions = [] \n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "            continue\n",
    "\n",
    "        target_indices = chunk_df.index\n",
    "        df.loc[target_indices, 'mobilitydata_labelled'] = predictions\n",
    "\n",
    "        key_request_counter += 1\n",
    "        time.sleep(0.8)\n",
    "        \n",
    "\n",
    "# After the main loop: retry failed attempts with chunk_size = 1\n",
    "error_df = df[df['mobilitydata_labelled'] == 'ERROR']\n",
    "\n",
    "if not error_df.empty:\n",
    "    print(f\"Retrying {len(error_df)} ERROR rows with chunk_size = 1\")\n",
    "\n",
    "    for group_name, group_df in error_df.groupby('iteration_index'):\n",
    "        print(f\"Retrying group {group_name} with {len(group_df)} rows\")\n",
    "        for i in tqdm(range(0, len(group_df), 1)):  # chunk_size = 1\n",
    "            if key_request_counter >= requests_per_key:\n",
    "                current_key_index += 1\n",
    "                key_request_counter = 0\n",
    "\n",
    "                if current_key_index >= key_count:\n",
    "                    elapsed = time.time() - cycle_start_time\n",
    "                    if elapsed < 60:\n",
    "                        wait_time = int(60 - elapsed)\n",
    "                        print(f\"Maximum requests per minute reached. Waiting {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    current_key_index = 0\n",
    "                    cycle_start_time = time.time()\n",
    "\n",
    "            CURRENT_API_KEY = API_KEYS[current_key_index]\n",
    "            client = genai.Client(api_key=CURRENT_API_KEY)\n",
    "\n",
    "            chunk_df = group_df.iloc[i:i + 1][relevant_columns]\n",
    "            formatter = group_chunk_lines[str(group_name)]\n",
    "            chunk_lines = chunk_df.apply(formatter, axis=1).tolist()\n",
    "\n",
    "            prompt = \"Handelt es sich bei folgendem Inhalt um Verkehrs- oder Mobilitätsdaten?Antworte nur mit T (True) oder F (False).\\n\\n\" + \"\\n\\n\".join(chunk_lines) + \"Antwort:\"\n",
    "    \n",
    "            response = client.models.generate_content_stream(\n",
    "                model=\"gemini-2.0-flash-lite-001\",\n",
    "                contents=[prompt],\n",
    "                config=types.GenerateContentConfig(\n",
    "                    max_output_tokens=1,\n",
    "                    temperature=0\n",
    "                )\n",
    "            )\n",
    "\n",
    "            result_text = \"\"\n",
    "            for chunk in response:\n",
    "                result_text += chunk.text\n",
    "\n",
    "            predictions = result_text.strip().splitlines()\n",
    "\n",
    "            if len(predictions) != 1:\n",
    "                df.loc[chunk_df.index, 'mobilitydata_labelled'] = 'ERROR'\n",
    "                continue\n",
    "\n",
    "            df.loc[chunk_df.index, 'mobilitydata_labelled'] = predictions\n",
    "            key_request_counter += 1\n",
    "            time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the LLM output ('T' or 'F') into actual boolean values (True/False)\n",
    "df['mobilitydata_labelled'] = df['mobilitydata_labelled'].map({'T': True, 'F': False})\n",
    "\n",
    "# Keep only rows with valid boolean labels (exclude 'ERROR', NaN, or anything else)\n",
    "df = df[df['mobilitydata_labelled'].isin([True, False])].copy()\n",
    "\n",
    "# Update the 'dataset_is_mobility' column in the DataFrame with the new boolean labels\n",
    "df = update_data(df, identifier_column=identifier, value_column='dataset_is_mobility', new_value=df['mobilitydata_labelled'])\n",
    "\n",
    "# Drop temporary helper columns used for internal processing\n",
    "df.drop(columns=['iteration_index', 'mobilitydata_labelled'], inplace=True)\n",
    "\n",
    "# Write the updated data back to the database using batch updates\n",
    "write_back_to_db(config, db_name, table, df, identifier_column=identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
