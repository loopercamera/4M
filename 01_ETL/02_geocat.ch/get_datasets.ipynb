{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 16221 records in steps of 100...\n",
      "Fetching records from 1 to 100...\n",
      "Fetching records from 101 to 200...\n",
      "Fetching records from 201 to 300...\n",
      "Fetching records from 301 to 400...\n",
      "Fetching records from 401 to 500...\n",
      "Fetching records from 501 to 600...\n",
      "Fetching records from 601 to 700...\n",
      "Fetching records from 701 to 800...\n",
      "Fetching records from 801 to 900...\n",
      "Fetching records from 901 to 1000...\n",
      "Fetching records from 1001 to 1100...\n",
      "Fetching records from 1101 to 1200...\n",
      "Fetching records from 1201 to 1300...\n",
      "Fetching records from 1301 to 1400...\n",
      "Fetching records from 1401 to 1500...\n",
      "Fetching records from 1501 to 1600...\n",
      "Fetching records from 1601 to 1700...\n",
      "Fetching records from 1701 to 1800...\n",
      "Fetching records from 1801 to 1900...\n",
      "Fetching records from 1901 to 2000...\n",
      "Fetching records from 2001 to 2100...\n",
      "Fetching records from 2101 to 2200...\n",
      "Fetching records from 2201 to 2300...\n",
      "Fetching records from 2301 to 2400...\n",
      "Fetching records from 2401 to 2500...\n",
      "Fetching records from 2501 to 2600...\n",
      "Fetching records from 2601 to 2700...\n",
      "Fetching records from 2701 to 2800...\n",
      "Fetching records from 2801 to 2900...\n",
      "Fetching records from 2901 to 3000...\n",
      "Fetching records from 3001 to 3100...\n",
      "Fetching records from 3101 to 3200...\n",
      "Fetching records from 3201 to 3300...\n",
      "Fetching records from 3301 to 3400...\n",
      "Fetching records from 3401 to 3500...\n",
      "Fetching records from 3501 to 3600...\n",
      "Fetching records from 3601 to 3700...\n",
      "Fetching records from 3701 to 3800...\n",
      "Fetching records from 3801 to 3900...\n",
      "Fetching records from 3901 to 4000...\n",
      "Fetching records from 4001 to 4100...\n",
      "Fetching records from 4101 to 4200...\n",
      "Fetching records from 4201 to 4300...\n",
      "Fetching records from 4301 to 4400...\n",
      "Fetching records from 4401 to 4500...\n",
      "Fetching records from 4501 to 4600...\n",
      "Fetching records from 4601 to 4700...\n",
      "Fetching records from 4701 to 4800...\n",
      "Fetching records from 4801 to 4900...\n",
      "Fetching records from 4901 to 5000...\n",
      "Fetching records from 5001 to 5100...\n",
      "Fetching records from 5101 to 5200...\n",
      "Fetching records from 5201 to 5300...\n",
      "Fetching records from 5301 to 5400...\n",
      "Fetching records from 5401 to 5500...\n",
      "Fetching records from 5501 to 5600...\n",
      "Fetching records from 5601 to 5700...\n",
      "Fetching records from 5701 to 5800...\n",
      "Fetching records from 5801 to 5900...\n",
      "Fetching records from 5901 to 6000...\n",
      "Fetching records from 6001 to 6100...\n",
      "Fetching records from 6101 to 6200...\n",
      "Fetching records from 6201 to 6300...\n",
      "Fetching records from 6301 to 6400...\n",
      "Fetching records from 6401 to 6500...\n",
      "Fetching records from 6501 to 6600...\n",
      "Fetching records from 6601 to 6700...\n",
      "Fetching records from 6701 to 6800...\n",
      "Fetching records from 6801 to 6900...\n",
      "Fetching records from 6901 to 7000...\n",
      "Fetching records from 7001 to 7100...\n",
      "Fetching records from 7101 to 7200...\n",
      "Fetching records from 7201 to 7300...\n",
      "Fetching records from 7301 to 7400...\n",
      "Fetching records from 7401 to 7500...\n",
      "Fetching records from 7501 to 7600...\n",
      "Fetching records from 7601 to 7700...\n",
      "Fetching records from 7701 to 7800...\n",
      "Fetching records from 7801 to 7900...\n",
      "Fetching records from 7901 to 8000...\n",
      "Fetching records from 8001 to 8100...\n",
      "Fetching records from 8101 to 8200...\n",
      "Fetching records from 8201 to 8300...\n",
      "Fetching records from 8301 to 8400...\n",
      "Fetching records from 8401 to 8500...\n",
      "Fetching records from 8501 to 8600...\n",
      "Fetching records from 8601 to 8700...\n",
      "Fetching records from 8701 to 8800...\n",
      "Fetching records from 8801 to 8900...\n",
      "Fetching records from 8901 to 9000...\n",
      "Fetching records from 9001 to 9100...\n",
      "Fetching records from 9101 to 9200...\n",
      "Fetching records from 9201 to 9300...\n",
      "Fetching records from 9301 to 9400...\n",
      "Fetching records from 9401 to 9500...\n",
      "Fetching records from 9501 to 9600...\n",
      "Fetching records from 9601 to 9700...\n",
      "Fetching records from 9701 to 9800...\n",
      "Fetching records from 9801 to 9900...\n",
      "Fetching records from 9901 to 10000...\n",
      "Fetching records from 10001 to 10100...\n",
      "Fetching records from 10101 to 10200...\n",
      "Fetching records from 10201 to 10300...\n",
      "Fetching records from 10301 to 10400...\n",
      "Fetching records from 10401 to 10500...\n",
      "Fetching records from 10501 to 10600...\n",
      "Fetching records from 10601 to 10700...\n",
      "Fetching records from 10701 to 10800...\n",
      "Fetching records from 10801 to 10900...\n",
      "Fetching records from 10901 to 11000...\n",
      "Fetching records from 11001 to 11100...\n",
      "Fetching records from 11101 to 11200...\n",
      "Fetching records from 11201 to 11300...\n",
      "Fetching records from 11301 to 11400...\n",
      "Fetching records from 11401 to 11500...\n",
      "Fetching records from 11501 to 11600...\n",
      "Fetching records from 11601 to 11700...\n",
      "Fetching records from 11701 to 11800...\n",
      "Fetching records from 11801 to 11900...\n",
      "Fetching records from 11901 to 12000...\n",
      "Fetching records from 12001 to 12100...\n",
      "Fetching records from 12101 to 12200...\n",
      "Fetching records from 12201 to 12300...\n",
      "Fetching records from 12301 to 12400...\n",
      "Fetching records from 12401 to 12500...\n",
      "Fetching records from 12501 to 12600...\n",
      "Fetching records from 12601 to 12700...\n",
      "Fetching records from 12701 to 12800...\n",
      "Fetching records from 12801 to 12900...\n",
      "Fetching records from 12901 to 13000...\n",
      "Fetching records from 13001 to 13100...\n",
      "Fetching records from 13101 to 13200...\n",
      "Fetching records from 13201 to 13300...\n",
      "Fetching records from 13301 to 13400...\n",
      "Fetching records from 13401 to 13500...\n",
      "Fetching records from 13501 to 13600...\n",
      "Fetching records from 13601 to 13700...\n",
      "Fetching records from 13701 to 13800...\n",
      "Fetching records from 13801 to 13900...\n",
      "Fetching records from 13901 to 14000...\n",
      "Fetching records from 14001 to 14100...\n",
      "Fetching records from 14101 to 14200...\n",
      "Fetching records from 14201 to 14300...\n",
      "Fetching records from 14301 to 14400...\n",
      "Fetching records from 14401 to 14500...\n",
      "Fetching records from 14501 to 14600...\n",
      "Fetching records from 14601 to 14700...\n",
      "Fetching records from 14701 to 14800...\n",
      "Fetching records from 14801 to 14900...\n",
      "Fetching records from 14901 to 15000...\n",
      "Fetching last 1221 records in descending order...\n",
      "Fetching reversed records from 1 to 100...\n",
      "Fetching reversed records from 101 to 200...\n",
      "Fetching reversed records from 201 to 300...\n",
      "Fetching reversed records from 301 to 400...\n",
      "Fetching reversed records from 401 to 500...\n",
      "Fetching reversed records from 501 to 600...\n",
      "Fetching reversed records from 601 to 700...\n",
      "Fetching reversed records from 701 to 800...\n",
      "Fetching reversed records from 801 to 900...\n",
      "Fetching reversed records from 901 to 1000...\n",
      "Fetching reversed records from 1001 to 1100...\n",
      "Fetching reversed records from 1101 to 1200...\n",
      "Fetching reversed records from 1201 to 1300...\n",
      "Saving 16200 records to CSV...\n",
      "Dataset saved to geocat_datasets.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Configuration Variables\n",
    "BATCH_SIZE = 100  # Number of records per request\n",
    "WAIT_TIME = 1  # Time in seconds between requests\n",
    "FETCH_LIMIT = None  # Set a fixed limit for testing, or None to fetch all datasets\n",
    "\n",
    "# CSW API Base URL\n",
    "CSW_URL = \"https://www.geocat.ch/geonetwork/srv/eng/csw\"\n",
    "\n",
    "# CSV output file\n",
    "OUTPUT_XML = \"geocat_datasets.csv\"\n",
    "\n",
    "# Function to get the total number of datasets\n",
    "def get_total_records():\n",
    "    params = {\n",
    "        \"service\": \"CSW\",\n",
    "        \"version\": \"2.0.2\",\n",
    "        \"request\": \"GetRecords\",\n",
    "        \"resultType\": \"results\",\n",
    "        \"outputFormat\": \"application/xml\",\n",
    "        \"outputSchema\": \"http://www.opengis.net/cat/csw/2.0.2\",\n",
    "        \"typeNames\": \"csw:Record\",\n",
    "        \"elementSetName\": \"brief\",\n",
    "        \"maxRecords\": 1,\n",
    "        \"startPosition\": 1,\n",
    "    }\n",
    "    response = requests.get(CSW_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    root = ET.fromstring(response.text)\n",
    "    namespaces = {\"csw\": \"http://www.opengis.net/cat/csw/2.0.2\"}\n",
    "    total_records = int(root.find(\".//csw:SearchResults\", namespaces).attrib.get(\"numberOfRecordsMatched\", 0))\n",
    "    return total_records\n",
    "\n",
    "# Function to fetch records\n",
    "def fetch_csw_records(start_position, sort_order=\"A\", max_records=BATCH_SIZE):\n",
    "    params = {\n",
    "        \"service\": \"CSW\",\n",
    "        \"version\": \"2.0.2\",\n",
    "        \"request\": \"GetRecords\",\n",
    "        \"resultType\": \"results\",\n",
    "        \"outputFormat\": \"application/xml\",\n",
    "        \"outputSchema\": \"http://www.opengis.net/cat/csw/2.0.2\",\n",
    "        \"typeNames\": \"csw:Record\",\n",
    "        \"elementSetName\": \"brief\",\n",
    "        \"maxRecords\": max_records,\n",
    "        \"startPosition\": start_position,\n",
    "        \"sortBy\": f\"title:{sort_order}\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(CSW_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "# Function to parse XML and extract dataset ID and Title\n",
    "def parse_csw_response(xml_data):\n",
    "    namespaces = {\n",
    "        \"csw\": \"http://www.opengis.net/cat/csw/2.0.2\",\n",
    "        \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "    }\n",
    "    root = ET.fromstring(xml_data)\n",
    "    records = []\n",
    "    for record in root.findall(\".//csw:BriefRecord\", namespaces):\n",
    "        dataset_id = record.find(\"dc:identifier\", namespaces).text.strip() if record.find(\"dc:identifier\", namespaces) is not None else \"N/A\"\n",
    "        dataset_title = record.find(\"dc:title\", namespaces).text.strip() if record.find(\"dc:title\", namespaces) is not None else \"N/A\"\n",
    "        records.append((dataset_id, dataset_title))\n",
    "    return records\n",
    "\n",
    "# Function to save to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Identifier\", \"Title\"])  # CSV Header\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    total_records = get_total_records()\n",
    "    if FETCH_LIMIT:\n",
    "        total_records = min(total_records, FETCH_LIMIT)  # Use FETCH_LIMIT if set\n",
    "\n",
    "    all_records = []\n",
    "    print(f\"Fetching {total_records} records in steps of {BATCH_SIZE}...\")\n",
    "    for start_pos in range(1, min(15001, total_records + 1), BATCH_SIZE):\n",
    "        print(f\"Fetching records from {start_pos} to {start_pos + BATCH_SIZE - 1}...\")\n",
    "        xml_response = fetch_csw_records(start_pos, sort_order=\"A\")\n",
    "        records = parse_csw_response(xml_response)\n",
    "        all_records.extend(records)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "    if total_records > 15000:\n",
    "        remaining_records = total_records - 15000\n",
    "        print(f\"Fetching last {remaining_records} records in descending order...\")\n",
    "        for start_pos in range(1, remaining_records + 1, BATCH_SIZE):\n",
    "            print(f\"Fetching reversed records from {start_pos} to {start_pos + BATCH_SIZE - 1}...\")\n",
    "            xml_response = fetch_csw_records(start_pos, sort_order=\"D\")\n",
    "            records = parse_csw_response(xml_response)\n",
    "            all_records.extend(records)\n",
    "            time.sleep(WAIT_TIME)\n",
    "\n",
    "    print(f\"Saving {len(all_records)} records to CSV...\")\n",
    "    save_to_csv(all_records, OUTPUT_XML)\n",
    "    print(f\"Dataset saved to {OUTPUT_XML}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request for 1000 records took: 0:04:18.592734\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 1000  # Number of records per request\n",
    "CSW_URL = \"https://www.geocat.ch/geonetwork/srv/eng/csw\"\n",
    "\n",
    "# Function to measure request time\n",
    "def measure_request_time(batch_size):\n",
    "    params = {\n",
    "        \"service\": \"CSW\",\n",
    "        \"version\": \"2.0.2\",\n",
    "        \"request\": \"GetRecords\",\n",
    "        \"resultType\": \"results\",\n",
    "        \"outputFormat\": \"application/xml\",\n",
    "        \"outputSchema\": \"http://www.opengis.net/cat/csw/2.0.2\",\n",
    "        \"typeNames\": \"csw:Record\",\n",
    "        \"elementSetName\": \"brief\",\n",
    "        \"maxRecords\": batch_size,\n",
    "        \"startPosition\": 1,\n",
    "    }\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    response = requests.get(CSW_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    elapsed_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"Request for {batch_size} records took: {elapsed_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    measure_request_time(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched metadata for geoportal-689-111 (Time taken: 0:00:00.296855)\n",
      "Fetched metadata for geoportal-152-69 (Time taken: 0:00:00.288492)\n",
      "Fetched metadata for afe12880-3dc3-8a66-94be-7cb3aa19529f (Time taken: 0:00:00.333897)\n",
      "Fetched metadata for a02205bf-c14f-4c0d-9fb4-cfaf700b1525 (Time taken: 0:00:00.289680)\n",
      "Fetched metadata for 9a5c3b20-8e56-8044-3df4-33b2a92b8316 (Time taken: 0:00:00.227687)\n",
      "Fetched metadata for geoportal-7-135 (Time taken: 0:00:00.246135)\n",
      "Fetched metadata for 6f90d5ba-c8aa-45ea-be6c-0e1c1b03dbd0 (Time taken: 0:00:00.317112)\n",
      "Fetched metadata for geoportal-951-78 (Time taken: 0:00:00.259755)\n",
      "Fetched metadata for 3d088b3c-4281-4d19-8169-9bbe6069eed0 (Time taken: 0:00:00.256784)\n",
      "Fetched metadata for geoportal-215-84-7993 (Time taken: 0:00:00.244221)\n",
      "Saved full metadata for 10 datasets to geocat_full_metadata.xml\n"
     ]
    }
   ],
   "source": [
    "https://www.geocat.ch/geonetwork/srv/api/records/geoportal-152-69/formatters/xml?approved=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data saved to: parsed_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    \"\"\"Parses an XML string and extracts data dynamically.\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string)\n",
    "        data = {}\n",
    "        for elem in root.iter():\n",
    "            if elem.tag not in data:  # Avoid duplicate keys\n",
    "                data[elem.tag] = elem.text.strip() if elem.text else None\n",
    "        return data\n",
    "    except ET.ParseError:\n",
    "        return {}\n",
    "\n",
    "def process_metadata(csv_input_path, csv_output_path):\n",
    "    \"\"\"Reads a CSV file, parses XML metadata, and saves structured data to a new CSV file.\"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_input_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Process all XML entries\n",
    "    parsed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        xml_content = row[\"Metadata\"]\n",
    "        parsed_entry = parse_xml(xml_content)\n",
    "        parsed_entry[\"Identifier\"] = row[\"Identifier\"]  # Retain original identifier\n",
    "        parsed_data.append(parsed_entry)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    parsed_df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    parsed_df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Parsed data saved to: {csv_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"geocat_full_metadata.csv\"  # Update with your file path\n",
    "    output_csv = \"parsed_metadata.csv\"\n",
    "    process_metadata(input_csv, output_csv)\n",
    "    parsed_metadata = pd.read_csv(\"parsed_metadata.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get XML form Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching metadata for 61ed4452-ac5b-4913-a3f8-fb5af1c0c157: 404 Client Error: Not Found for url: https://www.geocat.ch/geonetwork/srv/api/records/61ed4452-ac5b-4913-a3f8-fb5af1c0c157/formatters/xml?approved=true\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Fetch metadata\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m xml_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xml_data:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if request failed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mfetch_metadata\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m      8\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.geocat.ch/geonetwork/srv/api/records/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/formatters/xml?approved=true\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1061\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1064\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1065\u001b[0m         (\n\u001b[0;32m   1066\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1072\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    365\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def fetch_metadata(identifier):\n",
    "    \"\"\"Fetch XML metadata from GeoCat API with error handling.\"\"\"\n",
    "    url = f\"https://www.geocat.ch/geonetwork/srv/api/records/{identifier}/formatters/xml?approved=true\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching metadata for {identifier}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    \"\"\"Parse XML metadata and extract relevant data with error handling.\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string)\n",
    "        data = {}\n",
    "        for elem in root.iter():\n",
    "            if elem.tag not in data:\n",
    "                data[elem.tag] = elem.text.strip() if elem.text else None\n",
    "        return data\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_to_csv(df, filename=\"geocat_metadata.csv\"):\n",
    "    \"\"\"Save the merged DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved harvested metadata to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read dataset identifiers from CSV\n",
    "    datasets_df = pd.read_csv(\"geocat_datasets.csv\")\n",
    "    \n",
    "    # Select random datasets\n",
    "    sampled_datasets = datasets_df.sample(n=100, random_state=42)\n",
    "    \n",
    "    # List to store dataframes\n",
    "    dataframes = []\n",
    "    \n",
    "    # Fetch and store API responses for selected datasets\n",
    "    for _, row in sampled_datasets.iterrows():\n",
    "        identifier = row[\"Identifier\"]\n",
    "        title = row[\"Title\"]\n",
    "        timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Fetch metadata\n",
    "        xml_data = fetch_metadata(identifier)\n",
    "        if not xml_data:\n",
    "            continue  # Skip if request failed\n",
    "\n",
    "    \n",
    "\n",
    "        # Parse XML\n",
    "        parsed_data = parse_xml(xml_data)\n",
    "        parsed_data[\"Identifier\"] = identifier\n",
    "        parsed_data[\"Title\"] = title\n",
    "        parsed_data[\"Request_Timestamp\"] = timestamp\n",
    "    \n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([parsed_data])\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        save_to_csv(merged_df)\n",
    "    else:\n",
    "        print(\"No valid metadata retrieved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned metadata to geocat_metadata_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the parsed metadata CSV\n",
    "parsed_df = pd.read_csv(\"geocat_metadata.csv\")\n",
    "\n",
    "def remove_sparse_columns(df, threshold):\n",
    "    \"\"\"Remove columns where more than a certain percentage of values are NaN.\"\"\"\n",
    "    non_na_threshold = int(threshold * len(df))\n",
    "    df_cleaned = df.dropna(axis=1, thresh=non_na_threshold)\n",
    "    return df_cleaned\n",
    "\n",
    "# Set threshold for column removal\n",
    "sparse_threshold = 0.05  # Example: Remove columns with more than 50% NaN values\n",
    "\n",
    "# Apply the function\n",
    "cleaned_df = remove_sparse_columns(parsed_df, sparse_threshold)\n",
    "\n",
    "# Save cleaned data\n",
    "cleaned_df.to_csv(\"geocat_metadata_cleaned.csv\", index=False, encoding='utf-8')\n",
    "print(\"Saved cleaned metadata to geocat_metadata_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned XML response to saved_metadata_xml\\efbeacb0-f492-3fe6-6cd3-63127a09bf0b.xml\n",
      "Saved cleaned XML response to saved_metadata_xml\\da4696e6-c546-4a4f-bbf4-36d5d01c9e2f-6571.xml\n",
      "Saved cleaned XML response to saved_metadata_xml\\eef7063e-7c15-47f1-aa10-ce61f81778ad-6571.xml\n",
      "Saved cleaned XML response to saved_metadata_xml\\544e34c5-88b5-4c81-8290-05c3dd2f0a4f-6571.xml\n",
      "Saved cleaned XML response to saved_metadata_xml\\0e57f315-2b70-48c3-9802-4cde2db10c49-6571.xml\n",
      "Metadata retrieval completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configurations\n",
    "XML_SAVE_DIR = \"saved_metadata_xml\"\n",
    "os.makedirs(XML_SAVE_DIR, exist_ok=True)\n",
    "MAX_FILES = 1000  # Set the maximum number of files to download\n",
    "\n",
    "def fetch_and_save_metadata(identifier):\n",
    "    \"\"\"Fetch XML metadata from GeoCat API and save it to a folder without encoding issues or extra spaces.\"\"\"\n",
    "    url = f\"https://www.geocat.ch/geonetwork/srv/api/records/{identifier}/formatters/xml?approved=true\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Ensure correct encoding (UTF-8 handling)\n",
    "        xml_content = response.content.decode('utf-8')\n",
    "\n",
    "        # Normalize whitespace (removes excessive blank lines)\n",
    "        xml_content = '\\n'.join([line.strip() for line in xml_content.splitlines() if line.strip()])\n",
    "\n",
    "        # Save cleaned XML to file\n",
    "        xml_path = os.path.join(XML_SAVE_DIR, f\"{identifier}.xml\")\n",
    "        with open(xml_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(xml_content)\n",
    "        \n",
    "        print(f\"Saved cleaned XML response to {xml_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching metadata for {identifier}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the identifiers from the CSV file\n",
    "    dataset_file = \"geocat_datasets.csv\"\n",
    "    df_datasets = pd.read_csv(dataset_file)\n",
    "\n",
    "    # Process only up to MAX_FILES records\n",
    "    for index, row in df_datasets.iterrows():\n",
    "        if index >= MAX_FILES:\n",
    "            break  # Stop processing after reaching the limit\n",
    "\n",
    "        identifier = row[\"Identifier\"]\n",
    "        fetch_and_save_metadata(identifier)\n",
    "\n",
    "    print(\"Metadata retrieval completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
