{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for setting the quality control attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load configuration file:\", exception=e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_qc(config, limit=100):\n",
    "    import psycopg2\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            ds.dataset_identifier,\n",
    "             dist.distribution_format,\n",
    "             dist.distribution_download_url\n",
    "        FROM merged_dataset_metadata ds\n",
    "        LEFT JOIN merged_distribution_metadata dist\n",
    "            ON ds.dataset_identifier = dist.dataset_identifier\n",
    "        \"\"\"\n",
    "\n",
    "        if limit is not None:\n",
    "            query += f\" LIMIT {int(limit)}\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        if not df.empty:\n",
    "            print(\"Successfully loaded data out of the db\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error loading dataset metadata with distribution formats\", exception=e)\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"4M_copy\"\n",
    "config = load_config(r\"01_ETL\\21_load\\db_config.json\")\n",
    "config[\"dbname\"] = dbname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogenis format names of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\FHNW_lokal\\\\6000\\\\4M\\\\04_QC\\\\formats_lockup_utf8.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Lade Lookup-Tabelle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m lookup_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFHNW_lokal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m6000\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4M\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m04_QC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mformats_lockup_utf8.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m lookup_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlookup_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Bereinigen f√ºr stabile Zuordnung\u001b[39;00m\n\u001b[0;32m      8\u001b[0m lookup_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lookup_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\FHNW_lokal\\\\6000\\\\4M\\\\04_QC\\\\formats_lockup_utf8.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load lookup table\n",
    "lookup_path = r\"04_QC\\formats_lockup_utf8.csv\"\n",
    "lookup_df = pd.read_csv(lookup_path)\n",
    "\n",
    "# Clean up for stable matching\n",
    "lookup_df[\"original_name\"] = lookup_df[\"original_name\"].str.strip()\n",
    "\n",
    "# Load your database data as before\n",
    "df = fetch_data_qc(config, limit=None)\n",
    "\n",
    "# Clean and replace empty distribution_format values\n",
    "df[\"distribution_format\"] = df[\"distribution_format\"].astype(str).str.strip()\n",
    "df[\"distribution_format\"] = df[\"distribution_format\"].replace(\"\", \"no_information\")\n",
    "df[\"distribution_format\"] = df[\"distribution_format\"].fillna(\"no_information\")\n",
    "\n",
    "# Perform the merge with the lookup table\n",
    "df_merged = df.merge(\n",
    "    lookup_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"distribution_format\",\n",
    "    right_on=\"original_name\"\n",
    ")\n",
    "\n",
    "# Drop the now redundant original_name column\n",
    "df_merged = df_merged.drop(columns=[\"original_name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count format names in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  dataset_identifier  \\\n",
      "0    15f368b3-c660-4fcd-bec6-1413094d44bb@kanton-zug   \n",
      "1  8aeaff60-3351-4730-af28-7ec9813d2689@amt-fuer-...   \n",
      "2  8aeaff60-3351-4730-af28-7ec9813d2689@amt-fuer-...   \n",
      "3  2834dae3-266c-4aa0-8220-1eaea7a351ed@amt-geoin...   \n",
      "4                          100182@kanton-basel-stadt   \n",
      "\n",
      "                                 distribution_format  format_count  \n",
      "0  http://publications.europa.eu/resource/authori...             2  \n",
      "1  http://publications.europa.eu/resource/authori...             3  \n",
      "2  http://publications.europa.eu/resource/authori...             3  \n",
      "3  http://publications.europa.eu/resource/authori...             3  \n",
      "4  http://publications.europa.eu/resource/authori...            13  \n"
     ]
    }
   ],
   "source": [
    "# Filter formats that are not \"no_information\"\n",
    "df_valid_formats = df_merged[df_merged[\"distribution_format\"] != \"no_information\"]\n",
    "\n",
    "# Group and count the number of different formats per dataset\n",
    "format_counts = (\n",
    "    df_valid_formats.groupby(\"dataset_identifier\")[\"distribution_format\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"format_count\")\n",
    ")\n",
    "\n",
    "# Merge with original df_merged to also include datasets with 0 valid formats\n",
    "df_with_format_count = df_merged.merge(format_counts, on=\"dataset_identifier\", how=\"left\")\n",
    "\n",
    "# Replace missing count values with 0\n",
    "df_with_format_count[\"format_count\"] = df_with_format_count[\"format_count\"].fillna(0).astype(int)\n",
    "\n",
    "# Preview\n",
    "print(df_with_format_count[[\"dataset_identifier\", \"distribution_format\", \"format_count\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the distribution_download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking unique URLs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33100/33100 [2:54:59<00:00,  3.15it/s]   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_WORKERS = 30\n",
    "TIMEOUT = 5\n",
    "\n",
    "# Extract unique, cleaned URLs only\n",
    "unique_urls = df_merged[\"distribution_download_url\"].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Function to check the status of a single URL\n",
    "def check_url_status(url):\n",
    "    if not url:\n",
    "        return url, \"empty\"\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=TIMEOUT)\n",
    "        return url, response.status_code\n",
    "    except requests.exceptions.RequestException:\n",
    "        return url, \"error\"\n",
    "\n",
    "# Temporarily store results\n",
    "url_status_map = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(check_url_status, url): url for url in unique_urls}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Checking unique URLs\"):\n",
    "        url, status = future.result()\n",
    "        url_status_map[url] = status\n",
    "\n",
    "# Add column: status code based on mapping\n",
    "df_merged[\"download_url_status_code\"] = df_merged[\"distribution_download_url\"].astype(str).str.strip().map(url_status_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   download_url_status_code  count  \\\n",
      "0                       nan  33920   \n",
      "1                       200  32192   \n",
      "2                       403   2379   \n",
      "3                       405   2018   \n",
      "4                       404   1411   \n",
      "5                     error   1305   \n",
      "6                       400    541   \n",
      "7                       204    230   \n",
      "8                       503    100   \n",
      "9                       500     84   \n",
      "10                      401     18   \n",
      "\n",
      "                            distribution_download_url  \n",
      "0                                                None  \n",
      "1   https://data.bs.ch/api/v2/catalog/datasets/100...  \n",
      "2   https://www.baselland.ch/politik-und-behorden/...  \n",
      "3   https://wab.zug.ch/vote/ausbau-nationalstrasse...  \n",
      "4   https://wab.zug.ch/vote/anderung-vom-26.-septe...  \n",
      "5    https://mapplus01/mapplus/fribourg/?layers=11432  \n",
      "6   https://geoportal.georhena.eu/geoserver/transp...  \n",
      "7             https://data.zg.ch/store/1/resource/682  \n",
      "8   https://geo.ur.ch/sec-webmercator/wfs?request=...  \n",
      "9   https://map.geo.sz.ch/mapserv_proxy?SERVICE=WM...  \n",
      "10  https://wms.zh.ch/FalsIGWZHWMS?Service=WMS&Req...  \n"
     ]
    }
   ],
   "source": [
    "# Convert status codes to string\n",
    "df_merged[\"download_url_status_code\"] = df_merged[\"download_url_status_code\"].astype(str)\n",
    "\n",
    "# Count the number of occurrences per status code\n",
    "status_counts = (\n",
    "    df_merged[\"download_url_status_code\"]\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"download_url_status_code\", \"download_url_status_code\": \"count\"})\n",
    ")\n",
    "\n",
    "# Extract one example URL per status code\n",
    "example_urls = (\n",
    "    df_merged\n",
    "    .dropna(subset=[\"download_url_status_code\", \"distribution_download_url\"])\n",
    "    .drop_duplicates(subset=[\"download_url_status_code\"])\n",
    "    [[\"download_url_status_code\", \"distribution_download_url\"]]\n",
    ")\n",
    "\n",
    "# Merge both into a summary table\n",
    "status_overview = pd.merge(\n",
    "    status_counts,\n",
    "    example_urls,\n",
    "    on=\"download_url_status_code\",\n",
    "    how=\"left\"\n",
    ").sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display result\n",
    "print(status_overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_qc(config, limit=100):\n",
    "    import psycopg2\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            ds.dataset_identifier,\n",
    "             dist.distribution_format,\n",
    "             dist.distribution_download_url\n",
    "        FROM merged_dataset_metadata ds\n",
    "        LEFT JOIN merged_distribution_metadata dist\n",
    "            ON ds.dataset_identifier = dist.dataset_identifier\n",
    "        \"\"\"\n",
    "\n",
    "        if limit is not None:\n",
    "            query += f\" LIMIT {int(limit)}\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        if not df.empty:\n",
    "            print(\"Successfully loaded data out of the db\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error loading dataset metadata with distribution formats\", exception=e)\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT\\n    dataset_identifier,\\n    dataset_keyword_DE,\\n    CASE\\n        WHEN dataset_keyword_DE IS NOT NULL AND dataset_keyword_DE <> '' THEN\\n            array_length(string_to_array(trim(both '{}' from dataset_keyword_DE), ','), 1)\\n        ELSE 0\\n    END AS keyword_count\\nFROM merged_dataset_metadata;\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"SELECT\n",
    "    dataset_identifier,\n",
    "    dataset_keyword_DE,\n",
    "    CASE\n",
    "        WHEN dataset_keyword_DE IS NOT NULL AND dataset_keyword_DE <> '' THEN\n",
    "            array_length(string_to_array(trim(both '{}' from dataset_keyword_DE), ','), 1)\n",
    "        ELSE 0\n",
    "    END AS keyword_count\n",
    "FROM merged_dataset_metadata;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anzahl Zeichen in der Beschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT\\n    dataset_identifier,\\n    dataset_description_de,\\n    char_length(dataset_description_de) AS character_count\\nFROM merged_dataset_metadata\\nWHERE dataset_description_de IS NOT NULL\\nLIMIT 10;\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"SELECT\n",
    "    dataset_identifier,\n",
    "    dataset_description_de,\n",
    "    char_length(dataset_description_de) AS character_count\n",
    "FROM merged_dataset_metadata\n",
    "WHERE dataset_description_de IS NOT NULL\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
